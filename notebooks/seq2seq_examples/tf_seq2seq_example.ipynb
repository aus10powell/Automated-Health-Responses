{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for where this was taken from: * https://github.com/llSourcell/seq2seq_model_live/blob/master/2-seq2seq-advanced.ipynb\n",
    "* Also for reference: http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # matrix math\n",
    "import tensorflow as tf \n",
    "import helpers # don't have from video\n",
    "\n",
    "\n",
    "print('tf.__version__:',tf.__version__)\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining vocabulary size\n",
    "\n",
    "# What to padd the sequence when sequences are not the same length\n",
    "PAD = 0\n",
    "# WHen our sentences \n",
    "EOS = 1\n",
    "# Definining max length for input sequence\n",
    "vocab_size = 10\n",
    "#\n",
    "input_embedding_size = 20 # character length\n",
    "\n",
    "## Defining hidden units\n",
    "encoder_hidden_units = 20\n",
    "# Define the decoder to be a little different than the input...so multiply by 2\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialized embedding matrrix that can fit input sequence\n",
    "#used to convert sequences to vectors (embeddings) for both encoder and decoder of the right size\n",
    "#reshaping is a thing, in TF you gotta make sure you tensors are the right shape (num dimensions)\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tf.nn.embedding_lookup to index embedding matrix: given word 4, we represent it as 4th column of embedding matrix. This operation is lightweight, compared with alternative approach of one-hot encoding word 4 as [0,0,0,1,0,0,0,0,0,0] (vocab size 10) and then multiplying it by embedding matrix.\n",
    "\n",
    "Additionally, we don't need to compute gradients for any columns except 4th.\n",
    "\n",
    "**In real NLP application embedding matrix can get very large, with 100k or even 1m columns.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define encoder\n",
    "from tensorflow.python.ops.rnn_cell import LSTMStateTuple, LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get outputs and states\n",
    "#bidirectional RNN function takes a separate cell argument for \n",
    "#both the forward and backward RNN, and returns separate \n",
    "#outputs and states for both the forward and backward RNN\n",
    "\n",
    "#When using a standard RNN to make predictions we are only taking the “past” into account. \n",
    "#For certain tasks this makes sense (e.g. predicting the next word), but for some tasks \n",
    "#it would be useful to take both the past and the future into account. Think of a tagging task, \n",
    "#like part-of-speech tagging, where we want to assign a tag to each word in a sentence. \n",
    "#Here we already know the full sequence of words, and for each word we want to take not only the \n",
    "#words to the left (past) but also the words to the right (future) into account when making a prediction. \n",
    "#Bidirectional RNNs do exactly that. A bidirectional RNN is a combination of two RNNs – one runs forward from \n",
    "#“left to right” and one runs backward from “right to left”. These are commonly used for tagging tasks, or \n",
    "#when we want to embed a sequence into a fixed-length vector (beyond the scope of this post).\n",
    "\n",
    "\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentence:\n",
    "\"How are you?\"\n",
    "* Vectorize the sentence\n",
    "* Feed the vector to the encoder\n",
    "* **1 full time step, t:** encoder to hidden state\n",
    "    * Hidden state and previous are fed in to back propogation, so we are backpropogating through time\n",
    "* Feed that vector to decoder\n",
    "\n",
    "\n",
    "**It is better in every circumstance to have a bi-directional layer, but of course it is always going to be more computationally expensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining decoder: \n",
    "## Batches are subsets of the data.\n",
    "\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "# We want the decoder to be a little bigger than the input sequence. We don't yet know if the\n",
    "# sentence has ended yet.\n",
    "decoder_lengths = encoder_inputs_length + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output projection: Weights and biases\n",
    "## Going into more detail with the decoder as far as defining the decoder because we want to define\n",
    "## the weights manually.\n",
    "\n",
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine initializer and transition functions and create raw_rnn.\n",
    "\n",
    "Note that while all operations above are defined with TF's control flow and reduction ops, here we rely on checking if state is None to determine if it is an initializer call or transition call. This is not very clean API and might be changed in the future (indeed, tf.nn.raw_rnn's doc contains warning that API is experimental).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do output projection, we have to temporarilly flatten decoder_outputs from [max_steps, batch_size, hidden_dim] to [max_steps*batch_size, hidden_dim], as tf.matmul needs rank-2 tensors at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "\n",
    "\n",
    "RNN outputs tensor of shape [max_time, batch_size, hidden_units] which projection layer maps onto [max_time, batch_size, vocab_size]. vocab_size part of the shape is static, while max_time and batch_size is dynamic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-7d076de884f0>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the toy task\n",
    "\n",
    "Consider the copy task — given a random sequence of integers from a vocabulary, learn to memorize and reproduce input sequence. Because sequences are random, they do not contain any structure, unlike natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[7, 5, 8, 5, 6, 5, 9]\n",
      "[7, 4, 6, 4, 3, 6, 3, 8]\n",
      "[2, 9, 4]\n",
      "[9, 2, 2, 8, 5]\n",
      "[7, 3, 5]\n",
      "[2, 9, 3, 3, 3, 9, 5]\n",
      "[4, 3, 3, 7, 2, 8, 8, 3]\n",
      "[5, 2, 3, 3]\n",
      "[5, 6, 3, 7, 2, 7, 3, 5]\n",
      "[3, 7, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.372451066970825\n",
      "  sample 1:\n",
      "    input     > [7 6 9 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 6 8 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 7 6 4 9 3 5 5]\n",
      "    predicted > [5 5 5 5 5 5 6 8 6 8 2]\n",
      "  sample 3:\n",
      "    input     > [9 4 9 5 6 0 0 0]\n",
      "    predicted > [5 5 5 5 5 6 8 6 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.5512381792068481\n",
      "  sample 1:\n",
      "    input     > [7 7 3 5 9 5 6 0]\n",
      "    predicted > [7 7 3 5 5 5 6 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 3 8 3 5 6 0 0]\n",
      "    predicted > [3 3 3 8 5 6 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 8 6 0 0 0 0 0]\n",
      "    predicted > [5 8 6 1 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.27681127190589905\n",
      "  sample 1:\n",
      "    input     > [2 2 7 2 0 0 0 0]\n",
      "    predicted > [2 2 7 2 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 2 3 3 7 6 7 0]\n",
      "    predicted > [2 2 3 3 3 6 7 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 7 2 2 0 0 0 0]\n",
      "    predicted > [7 7 2 2 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.18458035588264465\n",
      "  sample 1:\n",
      "    input     > [2 8 4 8 5 0 0 0]\n",
      "    predicted > [2 8 4 8 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 9 8 5 6 5 0 0]\n",
      "    predicted > [5 9 8 5 6 5 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 4 2 0 0 0 0 0]\n",
      "    predicted > [8 4 2 1 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1853 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
