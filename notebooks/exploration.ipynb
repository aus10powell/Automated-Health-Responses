{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Chatbot with TF: https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077\n",
    "* Good reference list for reading: http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\n",
    "\n",
    "\n",
    "### Options:\n",
    "* Use categories from semtype tagging to identify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "import sys,os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500) # more columns displayed at once\n",
    "pd.options.display.max_colwidth = 200 # more of the text displayed at once\n",
    "\n",
    "# Custom\n",
    "from processing import tag_utterances\n",
    "from processing import load_sem_types\n",
    "\n",
    "## NLP\n",
    "import spacy\n",
    "\n",
    "# Set absolute path the QuickUMLS Server\n",
    "abs_path_umls = '/Users/austinpowell/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS'\n",
    "abs_path_data_umls = '/Users/austinpowell/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS_db'\n",
    "sys.path.append(abs_path_umls+'/QuickUMLS')\n",
    "from quickumls import QuickUMLS\n",
    "tagger = QuickUMLS(abs_path_data_umls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (557648, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for a manlet such as yourself I'd recommend at...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-Ai</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1513411674</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_7k5x2h</td>\n",
       "      <td>t3_7k5x2h</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514772e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>drbt2db</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you very much for answering!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-SY</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1445798103</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_3q697b</td>\n",
       "      <td>t1_cwcf958</td>\n",
       "      <td>2</td>\n",
       "      <td>1.447190e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cwcfjpr</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body score_hidden archived  \\\n",
       "0  for a manlet such as yourself I'd recommend at...          NaN      NaN   \n",
       "1                 Thank you very much for answering!          NaN      NaN   \n",
       "\n",
       "  name author                     author_flair_text  downs  created_utc  \\\n",
       "0  NaN    -Ai  This user has not yet been verified.    NaN   1513411674   \n",
       "1  NaN    -SY  This user has not yet been verified.    NaN   1445798103   \n",
       "\n",
       "  subreddit_id    link_id   parent_id  score  retrieved_on  controversiality  \\\n",
       "0     t5_2xtuc  t3_7k5x2h   t3_7k5x2h      0  1.514772e+09                 0   \n",
       "1     t5_2xtuc  t3_3q697b  t1_cwcf958      2  1.447190e+09                 0   \n",
       "\n",
       "   gilded       id subreddit  ups distinguished author_flair_css_class  \\\n",
       "0       0  drbt2db   AskDocs  NaN           NaN                default   \n",
       "1       0  cwcfjpr   AskDocs  2.0           NaN                default   \n",
       "\n",
       "   removal_reason  \n",
       "0             NaN  \n",
       "1             NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data = '../data/reddit_comments_askDocs_2014_to_2018_03.gz'\n",
    "df = pd.read_csv(path_to_data,low_memory=False)\n",
    "print('Shape',df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2661284537916392\n",
      "0.9484190743981866\n",
      "0.7033110492640519\n"
     ]
    }
   ],
   "source": [
    "print(len(df['link_id'].unique())/df.shape[0])\n",
    "print(len(df['id'].unique())/df.shape[0])\n",
    "print(len(df['parent_id'].unique())/df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary:\n",
    "Original post can be seen with the link_id\n",
    "* **link_id** Link to the page where the original thread was started\n",
    "    * **id**\n",
    "    * **parent_id** These ids are pointing back towards the original post and represent a new comment that is replying directly towards the original posting. Can be considered a general comment. I.e. All posts with the same parent_id (following the \"_\") as the link_id are. E.g. If the link_id is \"t3_827pgt\", all parent_id's with \"827pgt\" are pointing towards that original post.\n",
    "\n",
    "* **subreddit_id** and **subreddit** Irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename id columns to be more intuitive for a post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 22)\n",
      "(44, 22)\n",
      "(11, 22)\n"
     ]
    }
   ],
   "source": [
    "pdf.loc[df.parent_id.str.contains('827pgt',na=False)]rint(df.loc[df.link_id.str.contains('t3_827pgt',na=False)].shape)\n",
    "print(df.loc[df.link_id.str.contains('827pgt',na=False)].shape)\n",
    "\n",
    "print(.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 22)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.link_id.str.contains('827pgt',na=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47080</th>\n",
       "      <td>I'm not a doctor, but do you guys think this is Ankylosing Spondylitis?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yossi25</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1522308033</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.525707e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dwgoznj</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59878</th>\n",
       "      <td>Degenerative issues run in my family. From a young age, I was showing signs of DDD and it has progressed as the years go on. I wouldn't call my case severe, but enough to where I had spent a solid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vanteal</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520299064</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524857e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8rpap</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154535</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520316050</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.524865e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv95z1b</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189543</th>\n",
       "      <td>Im so sorry for your loss. I’m not a doctor so please don’t take this as fact as it’s based on my personal opinion and experience. My last MRI was pretty similar and I’ve been in pain 24 hours a d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twinkie45</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520287643</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>6</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8go5a</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195752</th>\n",
       "      <td>Sorry for your loss. It is never easy, she had a reason.\\nStill leaves a lot of pain behind.\\n\\nI have degeneration in my neck but it is nothing like what your mother may have had. As I recall the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DennyBenny</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520297740</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524856e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8qg82</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253902</th>\n",
       "      <td>It's not remotely the same but my father was about the same age as your mother, and in the end he chose not to go back to the hospital to treat his COPD, choosing to die at the seniors centre he w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hobbitlover</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520298169</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>6</td>\n",
       "      <td>1.524856e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8que6</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254645</th>\n",
       "      <td>She had degenerative disc and thinning disc from poor posture and old age. See pic: https://www-epainassist-com.cdn.ampproject.org/ii/w1000/s/www.epainassist.com/images/can-you-get-degenerative-di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jbjbjb55555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520310309</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.524862e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dv91xtu</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348588</th>\n",
       "      <td>Her fears of further degeneration and living in pain, would be My guess. Im sorry for your loss.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MDFrankensteen</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274122</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>45</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv81ye0</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390103</th>\n",
       "      <td>that looks alot like my MRI. I also have ddd. It's pretty painful and at times I have thought I didn't want to continue all the way out to the end of the disease because my pain is usually at leas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molotavcocktail</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520279487</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>18</td>\n",
       "      <td>1.524845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv87o36</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405258</th>\n",
       "      <td>So sorry for your loss.\\n\\nRoughly read through the report and from what I'm able to interpret it seems like your Mother is suffering from severe degeneration of the spine which resulted in multip...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sgmedicalstudent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274423</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>81</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv829t2</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444718</th>\n",
       "      <td>I hope that you find some peace, comfort, and understanding in these answers. I'm so sorry for your loss.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SerenityNOW_or_else_</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520286966</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>7</td>\n",
       "      <td>1.524850e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8fxdl</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           body  \\\n",
       "47080                                                                                                                                   I'm not a doctor, but do you guys think this is Ankylosing Spondylitis?   \n",
       "59878   Degenerative issues run in my family. From a young age, I was showing signs of DDD and it has progressed as the years go on. I wouldn't call my case severe, but enough to where I had spent a solid...   \n",
       "154535                                                                                                                                                                                                [deleted]   \n",
       "189543  Im so sorry for your loss. I’m not a doctor so please don’t take this as fact as it’s based on my personal opinion and experience. My last MRI was pretty similar and I’ve been in pain 24 hours a d...   \n",
       "195752  Sorry for your loss. It is never easy, she had a reason.\\nStill leaves a lot of pain behind.\\n\\nI have degeneration in my neck but it is nothing like what your mother may have had. As I recall the...   \n",
       "253902  It's not remotely the same but my father was about the same age as your mother, and in the end he chose not to go back to the hospital to treat his COPD, choosing to die at the seniors centre he w...   \n",
       "254645  She had degenerative disc and thinning disc from poor posture and old age. See pic: https://www-epainassist-com.cdn.ampproject.org/ii/w1000/s/www.epainassist.com/images/can-you-get-degenerative-di...   \n",
       "348588                                                                                                        Her fears of further degeneration and living in pain, would be My guess. Im sorry for your loss.    \n",
       "390103  that looks alot like my MRI. I also have ddd. It's pretty painful and at times I have thought I didn't want to continue all the way out to the end of the disease because my pain is usually at leas...   \n",
       "405258  So sorry for your loss.\\n\\nRoughly read through the report and from what I'm able to interpret it seems like your Mother is suffering from severe degeneration of the spine which resulted in multip...   \n",
       "444718                                                                                                I hope that you find some peace, comfort, and understanding in these answers. I'm so sorry for your loss.   \n",
       "\n",
       "       score_hidden archived name                author  \\\n",
       "47080           NaN      NaN  NaN               Yossi25   \n",
       "59878           NaN      NaN  NaN               vanteal   \n",
       "154535          NaN      NaN  NaN             [deleted]   \n",
       "189543          NaN      NaN  NaN             twinkie45   \n",
       "195752          NaN      NaN  NaN            DennyBenny   \n",
       "253902          NaN      NaN  NaN           hobbitlover   \n",
       "254645          NaN      NaN  NaN           jbjbjb55555   \n",
       "348588          NaN      NaN  NaN        MDFrankensteen   \n",
       "390103          NaN      NaN  NaN       molotavcocktail   \n",
       "405258          NaN      NaN  NaN      Sgmedicalstudent   \n",
       "444718          NaN      NaN  NaN  SerenityNOW_or_else_   \n",
       "\n",
       "                           author_flair_text  downs  created_utc subreddit_id  \\\n",
       "47080   This user has not yet been verified.    NaN   1522308033     t5_2xtuc   \n",
       "59878   This user has not yet been verified.    NaN   1520299064     t5_2xtuc   \n",
       "154535                                   NaN    NaN   1520316050     t5_2xtuc   \n",
       "189543  This user has not yet been verified.    NaN   1520287643     t5_2xtuc   \n",
       "195752  This user has not yet been verified.    NaN   1520297740     t5_2xtuc   \n",
       "253902                                   NaN    NaN   1520298169     t5_2xtuc   \n",
       "254645                                   NaN    NaN   1520310309     t5_2xtuc   \n",
       "348588  This user has not yet been verified.    NaN   1520274122     t5_2xtuc   \n",
       "390103  This user has not yet been verified.    NaN   1520279487     t5_2xtuc   \n",
       "405258                                   NaN    NaN   1520274423     t5_2xtuc   \n",
       "444718  This user has not yet been verified.    NaN   1520286966     t5_2xtuc   \n",
       "\n",
       "          link_id  parent_id  score  retrieved_on  controversiality  gilded  \\\n",
       "47080   t3_827pgt  t3_827pgt      1  1.525707e+09                 0       0   \n",
       "59878   t3_827pgt  t3_827pgt      5  1.524857e+09                 0       0   \n",
       "154535  t3_827pgt  t3_827pgt      1  1.524865e+09                 0       0   \n",
       "189543  t3_827pgt  t3_827pgt      6  1.524851e+09                 0       0   \n",
       "195752  t3_827pgt  t3_827pgt      2  1.524856e+09                 0       0   \n",
       "253902  t3_827pgt  t3_827pgt      6  1.524856e+09                 0       0   \n",
       "254645  t3_827pgt  t3_827pgt     -2  1.524862e+09                 1       0   \n",
       "348588  t3_827pgt  t3_827pgt     45  1.524842e+09                 0       0   \n",
       "390103  t3_827pgt  t3_827pgt     18  1.524845e+09                 0       0   \n",
       "405258  t3_827pgt  t3_827pgt     81  1.524842e+09                 0       0   \n",
       "444718  t3_827pgt  t3_827pgt      7  1.524850e+09                 0       0   \n",
       "\n",
       "             id subreddit  ups distinguished author_flair_css_class  \\\n",
       "47080   dwgoznj   AskDocs  NaN           NaN                default   \n",
       "59878   dv8rpap   AskDocs  NaN           NaN                default   \n",
       "154535  dv95z1b   AskDocs  NaN           NaN                    NaN   \n",
       "189543  dv8go5a   AskDocs  NaN           NaN                default   \n",
       "195752  dv8qg82   AskDocs  NaN           NaN                default   \n",
       "253902  dv8que6   AskDocs  NaN           NaN                    NaN   \n",
       "254645  dv91xtu   AskDocs  NaN           NaN                    NaN   \n",
       "348588  dv81ye0   AskDocs  NaN           NaN                default   \n",
       "390103  dv87o36   AskDocs  NaN           NaN                default   \n",
       "405258  dv829t2   AskDocs  NaN           NaN                    NaN   \n",
       "444718  dv8fxdl   AskDocs  NaN           NaN                default   \n",
       "\n",
       "        removal_reason      pd  \n",
       "47080              NaN  827pgt  \n",
       "59878              NaN  827pgt  \n",
       "154535             NaN  827pgt  \n",
       "189543             NaN  827pgt  \n",
       "195752             NaN  827pgt  \n",
       "253902             NaN  827pgt  \n",
       "254645             NaN  827pgt  \n",
       "348588             NaN  827pgt  \n",
       "390103             NaN  827pgt  \n",
       "405258             NaN  827pgt  \n",
       "444718             NaN  827pgt  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.parent_id.str.contains('827pgt',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223990</th>\n",
       "      <td>Wow, thank you for your story. I do not believe my mother was a coward in the slightest. She was a very strong woman and she did what she had to do. I’m not struggling against the grief, I’m float...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520302687</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8rpap</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524859e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8v4ny</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8rpap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           body  \\\n",
       "223990  Wow, thank you for your story. I do not believe my mother was a coward in the slightest. She was a very strong woman and she did what she had to do. I’m not struggling against the grief, I’m float...   \n",
       "\n",
       "       score_hidden archived name      author author_flair_text  downs  \\\n",
       "223990          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "\n",
       "        created_utc subreddit_id    link_id   parent_id  score  retrieved_on  \\\n",
       "223990   1520302687     t5_2xtuc  t3_827pgt  t1_dv8rpap      5  1.524859e+09   \n",
       "\n",
       "        controversiality  gilded       id subreddit  ups distinguished  \\\n",
       "223990                 0       0  dv8v4ny   AskDocs  NaN           NaN   \n",
       "\n",
       "       author_flair_css_class  removal_reason       pd  \n",
       "223990                    NaN             NaN  dv8rpap  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.parent_id.str.contains('dv8rpap',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226939</th>\n",
       "      <td>I can’t help with your particular question, but I just wanted to say that you seem like a very compassionate, kind, and strong person, and I really wish you the best. I’m so sorry for your loss.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pegmatitic</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520306013</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8v4ny</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524860e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8y923</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8v4ny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      body  \\\n",
       "226939  I can’t help with your particular question, but I just wanted to say that you seem like a very compassionate, kind, and strong person, and I really wish you the best. I’m so sorry for your loss.   \n",
       "\n",
       "       score_hidden archived name      author  \\\n",
       "226939          NaN      NaN  NaN  pegmatitic   \n",
       "\n",
       "                           author_flair_text  downs  created_utc subreddit_id  \\\n",
       "226939  This user has not yet been verified.    NaN   1520306013     t5_2xtuc   \n",
       "\n",
       "          link_id   parent_id  score  retrieved_on  controversiality  gilded  \\\n",
       "226939  t3_827pgt  t1_dv8v4ny      5  1.524860e+09                 0       0   \n",
       "\n",
       "             id subreddit  ups distinguished author_flair_css_class  \\\n",
       "226939  dv8y923   AskDocs  NaN           NaN                default   \n",
       "\n",
       "        removal_reason       pd  \n",
       "226939             NaN  dv8v4ny  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.parent_id.str.contains('dv8v4ny',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223992</th>\n",
       "      <td>Thank you very much. I'm not wondering why she did what she did. I absolutely understand why. I'm just looking for answers on what was wrong with her spine.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274235</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv81ye0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv822m9</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv81ye0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                body  \\\n",
       "223992  Thank you very much. I'm not wondering why she did what she did. I absolutely understand why. I'm just looking for answers on what was wrong with her spine.   \n",
       "\n",
       "       score_hidden archived name      author author_flair_text  downs  \\\n",
       "223992          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "\n",
       "        created_utc subreddit_id    link_id   parent_id  score  retrieved_on  \\\n",
       "223992   1520274235     t5_2xtuc  t3_827pgt  t1_dv81ye0      9  1.524842e+09   \n",
       "\n",
       "        controversiality  gilded       id subreddit  ups distinguished  \\\n",
       "223992                 0       0  dv822m9   AskDocs  NaN           NaN   \n",
       "\n",
       "       author_flair_css_class  removal_reason       pd  \n",
       "223992                    NaN             NaN  dv81ye0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.parent_id.str.contains('t1_dv81ye0',na=False)].loc[df['author']=='kiwikellie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223992</th>\n",
       "      <td>Thank you very much. I'm not wondering why she did what she did. I absolutely understand why. I'm just looking for answers on what was wrong with her spine.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274235</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv81ye0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv822m9</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv81ye0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348514</th>\n",
       "      <td>Degenerative disc disease.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MDFrankensteen</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274771</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv822m9</td>\n",
       "      <td>10</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv82n3m</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv822m9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121124</th>\n",
       "      <td>Reading through and I’m not a specialist in this field but patients I’ve had with similar impressions:\\n\\nNo matter what could’ve been done here she would be living with some degree of pain, which...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HokayeZeZ</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520285562</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv829t2</td>\n",
       "      <td>18</td>\n",
       "      <td>1.524849e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8ed1z</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv829t2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223985</th>\n",
       "      <td>Thank you. I believe she was told she would eventually become completely paralyzed. Do you believe that is something this condition could eventually progress to?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274604</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv829t2</td>\n",
       "      <td>35</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv82gtb</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv829t2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42816</th>\n",
       "      <td>Yes, unfortunately. It seems she had [myelomalacia](https://en.wikipedia.org/wiki/Myelomalacia) at the level around the third cervical vertebra (C3), which would eventually lead to paralysis of ev...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phhhhuh</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520357456</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv82gtb</td>\n",
       "      <td>3</td>\n",
       "      <td>1.524880e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv9x79p</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv82gtb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78045</th>\n",
       "      <td>If left untreated, yes.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pinky135</td>\n",
       "      <td>B.S., Medical Lab Sciences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520275503</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv82gtb</td>\n",
       "      <td>36</td>\n",
       "      <td>1.524843e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv83f4u</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>verified-mid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv82gtb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>Which isnt a disease, its a misnomer. We all have DDD but some worse than others.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jae_t</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520286469</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv82n3m</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.524850e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8fd0y</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv82n3m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223997</th>\n",
       "      <td>What would the treatment options have been for her? Would surgery have been out of the question due to her age?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520276053</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv83f4u</td>\n",
       "      <td>20</td>\n",
       "      <td>1.524843e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv840aq</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv83f4u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78071</th>\n",
       "      <td>I can't answer that. It depends on a lot of factors and only the treating doctor can give a treatment plan, if one is possible.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pinky135</td>\n",
       "      <td>B.S., Medical Lab Sciences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520276404</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv840aq</td>\n",
       "      <td>4</td>\n",
       "      <td>1.524843e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv84dtd</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>verified-mid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv840aq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273301</th>\n",
       "      <td>if left untreated it wouldve in ended in complete paralysis, only possible treatment is surgery, in a very high risk area, which might’ve ended up paralyzing her.\\n\\nI am so sorry for your loss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L_AlAbdallat</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520278032</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv840aq</td>\n",
       "      <td>33</td>\n",
       "      <td>1.524844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8643f</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv840aq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132308</th>\n",
       "      <td>She would almost certainly have been a decent surgical candidate based on the information you have provided. I’m not a spine surgeon, but as an anesthesiologist I take care of people with similar ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520286494</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv840aq</td>\n",
       "      <td>29</td>\n",
       "      <td>1.524850e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8fe0l</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv840aq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223986</th>\n",
       "      <td>Thank you. It definitely helps to learn as much about her condition as I can.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520278118</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8643f</td>\n",
       "      <td>14</td>\n",
       "      <td>1.524844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv867dt</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8643f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223993</th>\n",
       "      <td>Thank you for a more detailed explanation.\\n\\nI’m not upset at my mother for making the choice she did and I don’t accuse her of making a bad choice. She cared for my father through his cancer all...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520281114</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv87o36</td>\n",
       "      <td>24</td>\n",
       "      <td>1.524846e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv89g6u</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv87o36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Laying here due to severe Sciatica and Meralgia paresthetica. I seriously sympathize and empathize your pain. Bless you.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0MY</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520284462</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv87o36</td>\n",
       "      <td>8</td>\n",
       "      <td>1.524848e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8d5po</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv87o36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390101</th>\n",
       "      <td>such a tough thing to have to go through. It helps me to think my father is floating around me somehow. I had a strong dream abt him after a few weeks where he looked like the way he was in his pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molotavcocktail</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520289786</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv89g6u</td>\n",
       "      <td>7</td>\n",
       "      <td>1.524852e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8ivmu</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv89g6u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223989</th>\n",
       "      <td>I think she was already experiencing such pain and stiffness that the thought that it would get worse and worse was too overwhelming for her to bear. I think she made the best choice for herself.\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520288433</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8ed1z</td>\n",
       "      <td>8</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8hi3n</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8ed1z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223988</th>\n",
       "      <td>Can you please elaborate?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520288550</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8fd0y</td>\n",
       "      <td>3</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8hmg0</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8fd0y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223991</th>\n",
       "      <td>I’m glad to hear that she might have had another option, but also sad that I couldn’t convince her to talk to another doctor. She was so sure that there was nothing that could be done for her.\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520287993</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8fe0l</td>\n",
       "      <td>12</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8h1e9</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8fe0l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223995</th>\n",
       "      <td>Thank you for your kind words. I have been comforted that she probably made the best choice for herself. She lived very happily and had an active social life during her retirement in Florida. She ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520288823</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8go5a</td>\n",
       "      <td>10</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8hwgy</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8go5a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157602</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520310150</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8h1e9</td>\n",
       "      <td>12</td>\n",
       "      <td>1.524862e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv91tdd</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8h1e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271245</th>\n",
       "      <td>I was diagnosed with this in my lower back when I was a slightly overweight and inactive teenager. Fast forward 6 years of weight loss and belly dancing (which strengthens that area and the core m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FramingNoise</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520291169</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8hmg0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.524853e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8k8zn</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8hmg0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289040</th>\n",
       "      <td>I can try to explain what was wrong with her spine. \\n\\nSo our spine is made up of bones called vertebrae. In between each vertebrae is a \"disc\", which acts like a cushion for the bones and helps ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medschool201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520311727</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8hmg0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.524863e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv930hi</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8hmg0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223999</th>\n",
       "      <td>That’s such a nice way to think about your father. I hope you’re doing okay.\\n\\nThank you.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520289900</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8ivmu</td>\n",
       "      <td>7</td>\n",
       "      <td>1.524852e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8izqc</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8ivmu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223990</th>\n",
       "      <td>Wow, thank you for your story. I do not believe my mother was a coward in the slightest. She was a very strong woman and she did what she had to do. I’m not struggling against the grief, I’m float...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520302687</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8rpap</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524859e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8v4ny</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8rpap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226939</th>\n",
       "      <td>I can’t help with your particular question, but I just wanted to say that you seem like a very compassionate, kind, and strong person, and I really wish you the best. I’m so sorry for your loss.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pegmatitic</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520306013</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8v4ny</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524860e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8y923</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8v4ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223987</th>\n",
       "      <td>Thank you so much. We all do the best we can with what was handed to us. All the best for you, as well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520307281</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8y923</td>\n",
       "      <td>3</td>\n",
       "      <td>1.524861e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8zdsb</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8y923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223998</th>\n",
       "      <td>I struggled with guilt and regret for the first few days, but it’s honestly something that I don’t think about now.\\n\\nThank you.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520311737</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv91tdd</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524863e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv930qn</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv91tdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223994</th>\n",
       "      <td>Thank you for such an in-depth explanation. I really appreciate you taking the time to write this for me.\\n\\nI lost my father when I was 9 and now my mother at 29. I grew up understanding that my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520313546</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv930hi</td>\n",
       "      <td>13</td>\n",
       "      <td>1.524864e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv94al4</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv930hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144856</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520312515</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv930qn</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524863e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv93kvz</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv930qn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289041</th>\n",
       "      <td>It really does suck. Nobody deserves to lose a parent at age 9 and it's definitely not fair to lose both parents by age 29. \\n\\nIt sounds like you have had to learn how to take care of yourself an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medschool201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520314038</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv94al4</td>\n",
       "      <td>14</td>\n",
       "      <td>1.524864e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv94mjv</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv94al4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431216</th>\n",
       "      <td>For real. He/she is a true warrior. I really admire that</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iDontHaveKnowledge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520315463</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv94mjv</td>\n",
       "      <td>4</td>\n",
       "      <td>1.524864e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv95ky2</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv94mjv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114342</th>\n",
       "      <td>You are going to be an outstanding doctor.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youngmrs</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520318439</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv94mjv</td>\n",
       "      <td>11</td>\n",
       "      <td>1.524865e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv97e6r</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv94mjv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223996</th>\n",
       "      <td>Thank you. I’ll look into myelomalacia more.\\n\\nShe definitely had to make an unfair decision.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520358619</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv9x79p</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524880e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv9yhg6</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv9x79p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390103</th>\n",
       "      <td>that looks alot like my MRI. I also have ddd. It's pretty painful and at times I have thought I didn't want to continue all the way out to the end of the disease because my pain is usually at leas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molotavcocktail</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520279487</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>18</td>\n",
       "      <td>1.524845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv87o36</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47080</th>\n",
       "      <td>I'm not a doctor, but do you guys think this is Ankylosing Spondylitis?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yossi25</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1522308033</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.525707e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dwgoznj</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348588</th>\n",
       "      <td>Her fears of further degeneration and living in pain, would be My guess. Im sorry for your loss.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MDFrankensteen</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274122</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>45</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv81ye0</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405258</th>\n",
       "      <td>So sorry for your loss.\\n\\nRoughly read through the report and from what I'm able to interpret it seems like your Mother is suffering from severe degeneration of the spine which resulted in multip...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sgmedicalstudent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274423</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>81</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv829t2</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195752</th>\n",
       "      <td>Sorry for your loss. It is never easy, she had a reason.\\nStill leaves a lot of pain behind.\\n\\nI have degeneration in my neck but it is nothing like what your mother may have had. As I recall the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DennyBenny</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520297740</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524856e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8qg82</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254645</th>\n",
       "      <td>She had degenerative disc and thinning disc from poor posture and old age. See pic: https://www-epainassist-com.cdn.ampproject.org/ii/w1000/s/www.epainassist.com/images/can-you-get-degenerative-di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jbjbjb55555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520310309</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.524862e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dv91xtu</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253902</th>\n",
       "      <td>It's not remotely the same but my father was about the same age as your mother, and in the end he chose not to go back to the hospital to treat his COPD, choosing to die at the seniors centre he w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hobbitlover</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520298169</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>6</td>\n",
       "      <td>1.524856e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8que6</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154535</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520316050</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.524865e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv95z1b</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189543</th>\n",
       "      <td>Im so sorry for your loss. I’m not a doctor so please don’t take this as fact as it’s based on my personal opinion and experience. My last MRI was pretty similar and I’ve been in pain 24 hours a d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twinkie45</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520287643</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>6</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8go5a</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59878</th>\n",
       "      <td>Degenerative issues run in my family. From a young age, I was showing signs of DDD and it has progressed as the years go on. I wouldn't call my case severe, but enough to where I had spent a solid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vanteal</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520299064</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524857e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8rpap</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444718</th>\n",
       "      <td>I hope that you find some peace, comfort, and understanding in these answers. I'm so sorry for your loss.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SerenityNOW_or_else_</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520286966</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>7</td>\n",
       "      <td>1.524850e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8fxdl</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           body  \\\n",
       "223992                                             Thank you very much. I'm not wondering why she did what she did. I absolutely understand why. I'm just looking for answers on what was wrong with her spine.   \n",
       "348514                                                                                                                                                                              Degenerative disc disease.    \n",
       "121124  Reading through and I’m not a specialist in this field but patients I’ve had with similar impressions:\\n\\nNo matter what could’ve been done here she would be living with some degree of pain, which...   \n",
       "223985                                        Thank you. I believe she was told she would eventually become completely paralyzed. Do you believe that is something this condition could eventually progress to?   \n",
       "42816   Yes, unfortunately. It seems she had [myelomalacia](https://en.wikipedia.org/wiki/Myelomalacia) at the level around the third cervical vertebra (C3), which would eventually lead to paralysis of ev...   \n",
       "78045                                                                                                                                                                                   If left untreated, yes.   \n",
       "7937                                                                                                                          Which isnt a disease, its a misnomer. We all have DDD but some worse than others.   \n",
       "223997                                                                                          What would the treatment options have been for her? Would surgery have been out of the question due to her age?   \n",
       "78071                                                                           I can't answer that. It depends on a lot of factors and only the treating doctor can give a treatment plan, if one is possible.   \n",
       "273301        if left untreated it wouldve in ended in complete paralysis, only possible treatment is surgery, in a very high risk area, which might’ve ended up paralyzing her.\\n\\nI am so sorry for your loss   \n",
       "132308  She would almost certainly have been a decent surgical candidate based on the information you have provided. I’m not a spine surgeon, but as an anesthesiologist I take care of people with similar ...   \n",
       "223986                                                                                                                            Thank you. It definitely helps to learn as much about her condition as I can.   \n",
       "223993  Thank you for a more detailed explanation.\\n\\nI’m not upset at my mother for making the choice she did and I don’t accuse her of making a bad choice. She cared for my father through his cancer all...   \n",
       "9                                                                                      Laying here due to severe Sciatica and Meralgia paresthetica. I seriously sympathize and empathize your pain. Bless you.   \n",
       "390101  such a tough thing to have to go through. It helps me to think my father is floating around me somehow. I had a strong dream abt him after a few weeks where he looked like the way he was in his pr...   \n",
       "223989  I think she was already experiencing such pain and stiffness that the thought that it would get worse and worse was too overwhelming for her to bear. I think she made the best choice for herself.\\...   \n",
       "223988                                                                                                                                                                                Can you please elaborate?   \n",
       "223991  I’m glad to hear that she might have had another option, but also sad that I couldn’t convince her to talk to another doctor. She was so sure that there was nothing that could be done for her.\\n\\n...   \n",
       "223995  Thank you for your kind words. I have been comforted that she probably made the best choice for herself. She lived very happily and had an active social life during her retirement in Florida. She ...   \n",
       "157602                                                                                                                                                                                                [deleted]   \n",
       "271245  I was diagnosed with this in my lower back when I was a slightly overweight and inactive teenager. Fast forward 6 years of weight loss and belly dancing (which strengthens that area and the core m...   \n",
       "289040  I can try to explain what was wrong with her spine. \\n\\nSo our spine is made up of bones called vertebrae. In between each vertebrae is a \"disc\", which acts like a cushion for the bones and helps ...   \n",
       "223999                                                                                                               That’s such a nice way to think about your father. I hope you’re doing okay.\\n\\nThank you.   \n",
       "223990  Wow, thank you for your story. I do not believe my mother was a coward in the slightest. She was a very strong woman and she did what she had to do. I’m not struggling against the grief, I’m float...   \n",
       "226939       I can’t help with your particular question, but I just wanted to say that you seem like a very compassionate, kind, and strong person, and I really wish you the best. I’m so sorry for your loss.   \n",
       "223987                                                                                                  Thank you so much. We all do the best we can with what was handed to us. All the best for you, as well.   \n",
       "223998                                                                        I struggled with guilt and regret for the first few days, but it’s honestly something that I don’t think about now.\\n\\nThank you.   \n",
       "223994  Thank you for such an in-depth explanation. I really appreciate you taking the time to write this for me.\\n\\nI lost my father when I was 9 and now my mother at 29. I grew up understanding that my ...   \n",
       "144856                                                                                                                                                                                                [deleted]   \n",
       "289041  It really does suck. Nobody deserves to lose a parent at age 9 and it's definitely not fair to lose both parents by age 29. \\n\\nIt sounds like you have had to learn how to take care of yourself an...   \n",
       "431216                                                                                                                                                 For real. He/she is a true warrior. I really admire that   \n",
       "114342                                                                                                                                                               You are going to be an outstanding doctor.   \n",
       "223996                                                                                                           Thank you. I’ll look into myelomalacia more.\\n\\nShe definitely had to make an unfair decision.   \n",
       "390103  that looks alot like my MRI. I also have ddd. It's pretty painful and at times I have thought I didn't want to continue all the way out to the end of the disease because my pain is usually at leas...   \n",
       "47080                                                                                                                                   I'm not a doctor, but do you guys think this is Ankylosing Spondylitis?   \n",
       "348588                                                                                                        Her fears of further degeneration and living in pain, would be My guess. Im sorry for your loss.    \n",
       "405258  So sorry for your loss.\\n\\nRoughly read through the report and from what I'm able to interpret it seems like your Mother is suffering from severe degeneration of the spine which resulted in multip...   \n",
       "195752  Sorry for your loss. It is never easy, she had a reason.\\nStill leaves a lot of pain behind.\\n\\nI have degeneration in my neck but it is nothing like what your mother may have had. As I recall the...   \n",
       "254645  She had degenerative disc and thinning disc from poor posture and old age. See pic: https://www-epainassist-com.cdn.ampproject.org/ii/w1000/s/www.epainassist.com/images/can-you-get-degenerative-di...   \n",
       "253902  It's not remotely the same but my father was about the same age as your mother, and in the end he chose not to go back to the hospital to treat his COPD, choosing to die at the seniors centre he w...   \n",
       "154535                                                                                                                                                                                                [deleted]   \n",
       "189543  Im so sorry for your loss. I’m not a doctor so please don’t take this as fact as it’s based on my personal opinion and experience. My last MRI was pretty similar and I’ve been in pain 24 hours a d...   \n",
       "59878   Degenerative issues run in my family. From a young age, I was showing signs of DDD and it has progressed as the years go on. I wouldn't call my case severe, but enough to where I had spent a solid...   \n",
       "444718                                                                                                I hope that you find some peace, comfort, and understanding in these answers. I'm so sorry for your loss.   \n",
       "\n",
       "       score_hidden archived name                author  \\\n",
       "223992          NaN      NaN  NaN            kiwikellie   \n",
       "348514          NaN      NaN  NaN        MDFrankensteen   \n",
       "121124          NaN      NaN  NaN             HokayeZeZ   \n",
       "223985          NaN      NaN  NaN            kiwikellie   \n",
       "42816           NaN      NaN  NaN               Phhhhuh   \n",
       "78045           NaN      NaN  NaN              Pinky135   \n",
       "7937            NaN      NaN  NaN                 Jae_t   \n",
       "223997          NaN      NaN  NaN            kiwikellie   \n",
       "78071           NaN      NaN  NaN              Pinky135   \n",
       "273301          NaN      NaN  NaN          L_AlAbdallat   \n",
       "132308          NaN      NaN  NaN             [deleted]   \n",
       "223986          NaN      NaN  NaN            kiwikellie   \n",
       "223993          NaN      NaN  NaN            kiwikellie   \n",
       "9               NaN      NaN  NaN                   0MY   \n",
       "390101          NaN      NaN  NaN       molotavcocktail   \n",
       "223989          NaN      NaN  NaN            kiwikellie   \n",
       "223988          NaN      NaN  NaN            kiwikellie   \n",
       "223991          NaN      NaN  NaN            kiwikellie   \n",
       "223995          NaN      NaN  NaN            kiwikellie   \n",
       "157602          NaN      NaN  NaN             [deleted]   \n",
       "271245          NaN      NaN  NaN          FramingNoise   \n",
       "289040          NaN      NaN  NaN          medschool201   \n",
       "223999          NaN      NaN  NaN            kiwikellie   \n",
       "223990          NaN      NaN  NaN            kiwikellie   \n",
       "226939          NaN      NaN  NaN            pegmatitic   \n",
       "223987          NaN      NaN  NaN            kiwikellie   \n",
       "223998          NaN      NaN  NaN            kiwikellie   \n",
       "223994          NaN      NaN  NaN            kiwikellie   \n",
       "144856          NaN      NaN  NaN             [deleted]   \n",
       "289041          NaN      NaN  NaN          medschool201   \n",
       "431216          NaN      NaN  NaN    iDontHaveKnowledge   \n",
       "114342          NaN      NaN  NaN              youngmrs   \n",
       "223996          NaN      NaN  NaN            kiwikellie   \n",
       "390103          NaN      NaN  NaN       molotavcocktail   \n",
       "47080           NaN      NaN  NaN               Yossi25   \n",
       "348588          NaN      NaN  NaN        MDFrankensteen   \n",
       "405258          NaN      NaN  NaN      Sgmedicalstudent   \n",
       "195752          NaN      NaN  NaN            DennyBenny   \n",
       "254645          NaN      NaN  NaN           jbjbjb55555   \n",
       "253902          NaN      NaN  NaN           hobbitlover   \n",
       "154535          NaN      NaN  NaN             [deleted]   \n",
       "189543          NaN      NaN  NaN             twinkie45   \n",
       "59878           NaN      NaN  NaN               vanteal   \n",
       "444718          NaN      NaN  NaN  SerenityNOW_or_else_   \n",
       "\n",
       "                           author_flair_text  downs  created_utc subreddit_id  \\\n",
       "223992                                   NaN    NaN   1520274235     t5_2xtuc   \n",
       "348514  This user has not yet been verified.    NaN   1520274771     t5_2xtuc   \n",
       "121124  This user has not yet been verified.    NaN   1520285562     t5_2xtuc   \n",
       "223985                                   NaN    NaN   1520274604     t5_2xtuc   \n",
       "42816   This user has not yet been verified.    NaN   1520357456     t5_2xtuc   \n",
       "78045             B.S., Medical Lab Sciences    NaN   1520275503     t5_2xtuc   \n",
       "7937    This user has not yet been verified.    NaN   1520286469     t5_2xtuc   \n",
       "223997                                   NaN    NaN   1520276053     t5_2xtuc   \n",
       "78071             B.S., Medical Lab Sciences    NaN   1520276404     t5_2xtuc   \n",
       "273301  This user has not yet been verified.    NaN   1520278032     t5_2xtuc   \n",
       "132308                                   NaN    NaN   1520286494     t5_2xtuc   \n",
       "223986                                   NaN    NaN   1520278118     t5_2xtuc   \n",
       "223993                                   NaN    NaN   1520281114     t5_2xtuc   \n",
       "9       This user has not yet been verified.    NaN   1520284462     t5_2xtuc   \n",
       "390101  This user has not yet been verified.    NaN   1520289786     t5_2xtuc   \n",
       "223989                                   NaN    NaN   1520288433     t5_2xtuc   \n",
       "223988                                   NaN    NaN   1520288550     t5_2xtuc   \n",
       "223991                                   NaN    NaN   1520287993     t5_2xtuc   \n",
       "223995                                   NaN    NaN   1520288823     t5_2xtuc   \n",
       "157602                                   NaN    NaN   1520310150     t5_2xtuc   \n",
       "271245  This user has not yet been verified.    NaN   1520291169     t5_2xtuc   \n",
       "289040                                   NaN    NaN   1520311727     t5_2xtuc   \n",
       "223999                                   NaN    NaN   1520289900     t5_2xtuc   \n",
       "223990                                   NaN    NaN   1520302687     t5_2xtuc   \n",
       "226939  This user has not yet been verified.    NaN   1520306013     t5_2xtuc   \n",
       "223987                                   NaN    NaN   1520307281     t5_2xtuc   \n",
       "223998                                   NaN    NaN   1520311737     t5_2xtuc   \n",
       "223994                                   NaN    NaN   1520313546     t5_2xtuc   \n",
       "144856                                   NaN    NaN   1520312515     t5_2xtuc   \n",
       "289041                                   NaN    NaN   1520314038     t5_2xtuc   \n",
       "431216                                   NaN    NaN   1520315463     t5_2xtuc   \n",
       "114342  This user has not yet been verified.    NaN   1520318439     t5_2xtuc   \n",
       "223996                                   NaN    NaN   1520358619     t5_2xtuc   \n",
       "390103  This user has not yet been verified.    NaN   1520279487     t5_2xtuc   \n",
       "47080   This user has not yet been verified.    NaN   1522308033     t5_2xtuc   \n",
       "348588  This user has not yet been verified.    NaN   1520274122     t5_2xtuc   \n",
       "405258                                   NaN    NaN   1520274423     t5_2xtuc   \n",
       "195752  This user has not yet been verified.    NaN   1520297740     t5_2xtuc   \n",
       "254645                                   NaN    NaN   1520310309     t5_2xtuc   \n",
       "253902                                   NaN    NaN   1520298169     t5_2xtuc   \n",
       "154535                                   NaN    NaN   1520316050     t5_2xtuc   \n",
       "189543  This user has not yet been verified.    NaN   1520287643     t5_2xtuc   \n",
       "59878   This user has not yet been verified.    NaN   1520299064     t5_2xtuc   \n",
       "444718  This user has not yet been verified.    NaN   1520286966     t5_2xtuc   \n",
       "\n",
       "          link_id   parent_id  score  retrieved_on  controversiality  gilded  \\\n",
       "223992  t3_827pgt  t1_dv81ye0      9  1.524842e+09                 0       0   \n",
       "348514  t3_827pgt  t1_dv822m9     10  1.524842e+09                 0       0   \n",
       "121124  t3_827pgt  t1_dv829t2     18  1.524849e+09                 0       0   \n",
       "223985  t3_827pgt  t1_dv829t2     35  1.524842e+09                 0       0   \n",
       "42816   t3_827pgt  t1_dv82gtb      3  1.524880e+09                 0       0   \n",
       "78045   t3_827pgt  t1_dv82gtb     36  1.524843e+09                 0       0   \n",
       "7937    t3_827pgt  t1_dv82n3m     -5  1.524850e+09                 1       0   \n",
       "223997  t3_827pgt  t1_dv83f4u     20  1.524843e+09                 0       0   \n",
       "78071   t3_827pgt  t1_dv840aq      4  1.524843e+09                 0       0   \n",
       "273301  t3_827pgt  t1_dv840aq     33  1.524844e+09                 0       0   \n",
       "132308  t3_827pgt  t1_dv840aq     29  1.524850e+09                 0       0   \n",
       "223986  t3_827pgt  t1_dv8643f     14  1.524844e+09                 0       0   \n",
       "223993  t3_827pgt  t1_dv87o36     24  1.524846e+09                 0       0   \n",
       "9       t3_827pgt  t1_dv87o36      8  1.524848e+09                 0       0   \n",
       "390101  t3_827pgt  t1_dv89g6u      7  1.524852e+09                 0       0   \n",
       "223989  t3_827pgt  t1_dv8ed1z      8  1.524851e+09                 0       0   \n",
       "223988  t3_827pgt  t1_dv8fd0y      3  1.524851e+09                 0       0   \n",
       "223991  t3_827pgt  t1_dv8fe0l     12  1.524851e+09                 0       0   \n",
       "223995  t3_827pgt  t1_dv8go5a     10  1.524851e+09                 0       0   \n",
       "157602  t3_827pgt  t1_dv8h1e9     12  1.524862e+09                 0       0   \n",
       "271245  t3_827pgt  t1_dv8hmg0      3  1.524853e+09                 0       0   \n",
       "289040  t3_827pgt  t1_dv8hmg0     13  1.524863e+09                 0       0   \n",
       "223999  t3_827pgt  t1_dv8ivmu      7  1.524852e+09                 0       0   \n",
       "223990  t3_827pgt  t1_dv8rpap      5  1.524859e+09                 0       0   \n",
       "226939  t3_827pgt  t1_dv8v4ny      5  1.524860e+09                 0       0   \n",
       "223987  t3_827pgt  t1_dv8y923      3  1.524861e+09                 0       0   \n",
       "223998  t3_827pgt  t1_dv91tdd      5  1.524863e+09                 0       0   \n",
       "223994  t3_827pgt  t1_dv930hi     13  1.524864e+09                 0       0   \n",
       "144856  t3_827pgt  t1_dv930qn      2  1.524863e+09                 0       0   \n",
       "289041  t3_827pgt  t1_dv94al4     14  1.524864e+09                 0       0   \n",
       "431216  t3_827pgt  t1_dv94mjv      4  1.524864e+09                 0       0   \n",
       "114342  t3_827pgt  t1_dv94mjv     11  1.524865e+09                 0       0   \n",
       "223996  t3_827pgt  t1_dv9x79p      2  1.524880e+09                 0       0   \n",
       "390103  t3_827pgt   t3_827pgt     18  1.524845e+09                 0       0   \n",
       "47080   t3_827pgt   t3_827pgt      1  1.525707e+09                 0       0   \n",
       "348588  t3_827pgt   t3_827pgt     45  1.524842e+09                 0       0   \n",
       "405258  t3_827pgt   t3_827pgt     81  1.524842e+09                 0       0   \n",
       "195752  t3_827pgt   t3_827pgt      2  1.524856e+09                 0       0   \n",
       "254645  t3_827pgt   t3_827pgt     -2  1.524862e+09                 1       0   \n",
       "253902  t3_827pgt   t3_827pgt      6  1.524856e+09                 0       0   \n",
       "154535  t3_827pgt   t3_827pgt      1  1.524865e+09                 0       0   \n",
       "189543  t3_827pgt   t3_827pgt      6  1.524851e+09                 0       0   \n",
       "59878   t3_827pgt   t3_827pgt      5  1.524857e+09                 0       0   \n",
       "444718  t3_827pgt   t3_827pgt      7  1.524850e+09                 0       0   \n",
       "\n",
       "             id subreddit  ups distinguished author_flair_css_class  \\\n",
       "223992  dv822m9   AskDocs  NaN           NaN                    NaN   \n",
       "348514  dv82n3m   AskDocs  NaN           NaN                default   \n",
       "121124  dv8ed1z   AskDocs  NaN           NaN                default   \n",
       "223985  dv82gtb   AskDocs  NaN           NaN                    NaN   \n",
       "42816   dv9x79p   AskDocs  NaN           NaN                default   \n",
       "78045   dv83f4u   AskDocs  NaN           NaN           verified-mid   \n",
       "7937    dv8fd0y   AskDocs  NaN           NaN                default   \n",
       "223997  dv840aq   AskDocs  NaN           NaN                    NaN   \n",
       "78071   dv84dtd   AskDocs  NaN           NaN           verified-mid   \n",
       "273301  dv8643f   AskDocs  NaN           NaN                default   \n",
       "132308  dv8fe0l   AskDocs  NaN           NaN                    NaN   \n",
       "223986  dv867dt   AskDocs  NaN           NaN                    NaN   \n",
       "223993  dv89g6u   AskDocs  NaN           NaN                    NaN   \n",
       "9       dv8d5po   AskDocs  NaN           NaN                default   \n",
       "390101  dv8ivmu   AskDocs  NaN           NaN                default   \n",
       "223989  dv8hi3n   AskDocs  NaN           NaN                    NaN   \n",
       "223988  dv8hmg0   AskDocs  NaN           NaN                    NaN   \n",
       "223991  dv8h1e9   AskDocs  NaN           NaN                    NaN   \n",
       "223995  dv8hwgy   AskDocs  NaN           NaN                    NaN   \n",
       "157602  dv91tdd   AskDocs  NaN           NaN                    NaN   \n",
       "271245  dv8k8zn   AskDocs  NaN           NaN                default   \n",
       "289040  dv930hi   AskDocs  NaN           NaN                    NaN   \n",
       "223999  dv8izqc   AskDocs  NaN           NaN                    NaN   \n",
       "223990  dv8v4ny   AskDocs  NaN           NaN                    NaN   \n",
       "226939  dv8y923   AskDocs  NaN           NaN                default   \n",
       "223987  dv8zdsb   AskDocs  NaN           NaN                    NaN   \n",
       "223998  dv930qn   AskDocs  NaN           NaN                    NaN   \n",
       "223994  dv94al4   AskDocs  NaN           NaN                    NaN   \n",
       "144856  dv93kvz   AskDocs  NaN           NaN                    NaN   \n",
       "289041  dv94mjv   AskDocs  NaN           NaN                    NaN   \n",
       "431216  dv95ky2   AskDocs  NaN           NaN                    NaN   \n",
       "114342  dv97e6r   AskDocs  NaN           NaN                default   \n",
       "223996  dv9yhg6   AskDocs  NaN           NaN                    NaN   \n",
       "390103  dv87o36   AskDocs  NaN           NaN                default   \n",
       "47080   dwgoznj   AskDocs  NaN           NaN                default   \n",
       "348588  dv81ye0   AskDocs  NaN           NaN                default   \n",
       "405258  dv829t2   AskDocs  NaN           NaN                    NaN   \n",
       "195752  dv8qg82   AskDocs  NaN           NaN                default   \n",
       "254645  dv91xtu   AskDocs  NaN           NaN                    NaN   \n",
       "253902  dv8que6   AskDocs  NaN           NaN                    NaN   \n",
       "154535  dv95z1b   AskDocs  NaN           NaN                    NaN   \n",
       "189543  dv8go5a   AskDocs  NaN           NaN                default   \n",
       "59878   dv8rpap   AskDocs  NaN           NaN                default   \n",
       "444718  dv8fxdl   AskDocs  NaN           NaN                default   \n",
       "\n",
       "        removal_reason       pd  \n",
       "223992             NaN  dv81ye0  \n",
       "348514             NaN  dv822m9  \n",
       "121124             NaN  dv829t2  \n",
       "223985             NaN  dv829t2  \n",
       "42816              NaN  dv82gtb  \n",
       "78045              NaN  dv82gtb  \n",
       "7937               NaN  dv82n3m  \n",
       "223997             NaN  dv83f4u  \n",
       "78071              NaN  dv840aq  \n",
       "273301             NaN  dv840aq  \n",
       "132308             NaN  dv840aq  \n",
       "223986             NaN  dv8643f  \n",
       "223993             NaN  dv87o36  \n",
       "9                  NaN  dv87o36  \n",
       "390101             NaN  dv89g6u  \n",
       "223989             NaN  dv8ed1z  \n",
       "223988             NaN  dv8fd0y  \n",
       "223991             NaN  dv8fe0l  \n",
       "223995             NaN  dv8go5a  \n",
       "157602             NaN  dv8h1e9  \n",
       "271245             NaN  dv8hmg0  \n",
       "289040             NaN  dv8hmg0  \n",
       "223999             NaN  dv8ivmu  \n",
       "223990             NaN  dv8rpap  \n",
       "226939             NaN  dv8v4ny  \n",
       "223987             NaN  dv8y923  \n",
       "223998             NaN  dv91tdd  \n",
       "223994             NaN  dv930hi  \n",
       "144856             NaN  dv930qn  \n",
       "289041             NaN  dv94al4  \n",
       "431216             NaN  dv94mjv  \n",
       "114342             NaN  dv94mjv  \n",
       "223996             NaN  dv9x79p  \n",
       "390103             NaN   827pgt  \n",
       "47080              NaN   827pgt  \n",
       "348588             NaN   827pgt  \n",
       "405258             NaN   827pgt  \n",
       "195752             NaN   827pgt  \n",
       "254645             NaN   827pgt  \n",
       "253902             NaN   827pgt  \n",
       "154535             NaN   827pgt  \n",
       "189543             NaN   827pgt  \n",
       "59878              NaN   827pgt  \n",
       "444718             NaN   827pgt  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.link_id.str.contains('t3_827pgt',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223985</th>\n",
       "      <td>Thank you. I believe she was told she would eventually become completely paralyzed. Do you believe that is something this condition could eventually progress to?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274604</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv829t2</td>\n",
       "      <td>35</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv82gtb</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv829t2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223986</th>\n",
       "      <td>Thank you. It definitely helps to learn as much about her condition as I can.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520278118</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8643f</td>\n",
       "      <td>14</td>\n",
       "      <td>1.524844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv867dt</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8643f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223987</th>\n",
       "      <td>Thank you so much. We all do the best we can with what was handed to us. All the best for you, as well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520307281</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8y923</td>\n",
       "      <td>3</td>\n",
       "      <td>1.524861e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8zdsb</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8y923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223988</th>\n",
       "      <td>Can you please elaborate?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520288550</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8fd0y</td>\n",
       "      <td>3</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8hmg0</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8fd0y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223989</th>\n",
       "      <td>I think she was already experiencing such pain and stiffness that the thought that it would get worse and worse was too overwhelming for her to bear. I think she made the best choice for herself.\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520288433</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8ed1z</td>\n",
       "      <td>8</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8hi3n</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8ed1z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223990</th>\n",
       "      <td>Wow, thank you for your story. I do not believe my mother was a coward in the slightest. She was a very strong woman and she did what she had to do. I’m not struggling against the grief, I’m float...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520302687</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8rpap</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524859e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8v4ny</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8rpap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223991</th>\n",
       "      <td>I’m glad to hear that she might have had another option, but also sad that I couldn’t convince her to talk to another doctor. She was so sure that there was nothing that could be done for her.\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520287993</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8fe0l</td>\n",
       "      <td>12</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8h1e9</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8fe0l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223992</th>\n",
       "      <td>Thank you very much. I'm not wondering why she did what she did. I absolutely understand why. I'm just looking for answers on what was wrong with her spine.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274235</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv81ye0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv822m9</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv81ye0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223993</th>\n",
       "      <td>Thank you for a more detailed explanation.\\n\\nI’m not upset at my mother for making the choice she did and I don’t accuse her of making a bad choice. She cared for my father through his cancer all...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520281114</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv87o36</td>\n",
       "      <td>24</td>\n",
       "      <td>1.524846e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv89g6u</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv87o36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223994</th>\n",
       "      <td>Thank you for such an in-depth explanation. I really appreciate you taking the time to write this for me.\\n\\nI lost my father when I was 9 and now my mother at 29. I grew up understanding that my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520313546</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv930hi</td>\n",
       "      <td>13</td>\n",
       "      <td>1.524864e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv94al4</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv930hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223995</th>\n",
       "      <td>Thank you for your kind words. I have been comforted that she probably made the best choice for herself. She lived very happily and had an active social life during her retirement in Florida. She ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520288823</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8go5a</td>\n",
       "      <td>10</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8hwgy</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8go5a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223996</th>\n",
       "      <td>Thank you. I’ll look into myelomalacia more.\\n\\nShe definitely had to make an unfair decision.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520358619</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv9x79p</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524880e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv9yhg6</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv9x79p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223997</th>\n",
       "      <td>What would the treatment options have been for her? Would surgery have been out of the question due to her age?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520276053</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv83f4u</td>\n",
       "      <td>20</td>\n",
       "      <td>1.524843e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv840aq</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv83f4u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223998</th>\n",
       "      <td>I struggled with guilt and regret for the first few days, but it’s honestly something that I don’t think about now.\\n\\nThank you.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520311737</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv91tdd</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524863e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv930qn</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv91tdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223999</th>\n",
       "      <td>That’s such a nice way to think about your father. I hope you’re doing okay.\\n\\nThank you.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kiwikellie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520289900</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t1_dv8ivmu</td>\n",
       "      <td>7</td>\n",
       "      <td>1.524852e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8izqc</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dv8ivmu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           body  \\\n",
       "223985                                        Thank you. I believe she was told she would eventually become completely paralyzed. Do you believe that is something this condition could eventually progress to?   \n",
       "223986                                                                                                                            Thank you. It definitely helps to learn as much about her condition as I can.   \n",
       "223987                                                                                                  Thank you so much. We all do the best we can with what was handed to us. All the best for you, as well.   \n",
       "223988                                                                                                                                                                                Can you please elaborate?   \n",
       "223989  I think she was already experiencing such pain and stiffness that the thought that it would get worse and worse was too overwhelming for her to bear. I think she made the best choice for herself.\\...   \n",
       "223990  Wow, thank you for your story. I do not believe my mother was a coward in the slightest. She was a very strong woman and she did what she had to do. I’m not struggling against the grief, I’m float...   \n",
       "223991  I’m glad to hear that she might have had another option, but also sad that I couldn’t convince her to talk to another doctor. She was so sure that there was nothing that could be done for her.\\n\\n...   \n",
       "223992                                             Thank you very much. I'm not wondering why she did what she did. I absolutely understand why. I'm just looking for answers on what was wrong with her spine.   \n",
       "223993  Thank you for a more detailed explanation.\\n\\nI’m not upset at my mother for making the choice she did and I don’t accuse her of making a bad choice. She cared for my father through his cancer all...   \n",
       "223994  Thank you for such an in-depth explanation. I really appreciate you taking the time to write this for me.\\n\\nI lost my father when I was 9 and now my mother at 29. I grew up understanding that my ...   \n",
       "223995  Thank you for your kind words. I have been comforted that she probably made the best choice for herself. She lived very happily and had an active social life during her retirement in Florida. She ...   \n",
       "223996                                                                                                           Thank you. I’ll look into myelomalacia more.\\n\\nShe definitely had to make an unfair decision.   \n",
       "223997                                                                                          What would the treatment options have been for her? Would surgery have been out of the question due to her age?   \n",
       "223998                                                                        I struggled with guilt and regret for the first few days, but it’s honestly something that I don’t think about now.\\n\\nThank you.   \n",
       "223999                                                                                                               That’s such a nice way to think about your father. I hope you’re doing okay.\\n\\nThank you.   \n",
       "\n",
       "       score_hidden archived name      author author_flair_text  downs  \\\n",
       "223985          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223986          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223987          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223988          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223989          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223990          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223991          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223992          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223993          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223994          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223995          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223996          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223997          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223998          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "223999          NaN      NaN  NaN  kiwikellie               NaN    NaN   \n",
       "\n",
       "        created_utc subreddit_id    link_id   parent_id  score  retrieved_on  \\\n",
       "223985   1520274604     t5_2xtuc  t3_827pgt  t1_dv829t2     35  1.524842e+09   \n",
       "223986   1520278118     t5_2xtuc  t3_827pgt  t1_dv8643f     14  1.524844e+09   \n",
       "223987   1520307281     t5_2xtuc  t3_827pgt  t1_dv8y923      3  1.524861e+09   \n",
       "223988   1520288550     t5_2xtuc  t3_827pgt  t1_dv8fd0y      3  1.524851e+09   \n",
       "223989   1520288433     t5_2xtuc  t3_827pgt  t1_dv8ed1z      8  1.524851e+09   \n",
       "223990   1520302687     t5_2xtuc  t3_827pgt  t1_dv8rpap      5  1.524859e+09   \n",
       "223991   1520287993     t5_2xtuc  t3_827pgt  t1_dv8fe0l     12  1.524851e+09   \n",
       "223992   1520274235     t5_2xtuc  t3_827pgt  t1_dv81ye0      9  1.524842e+09   \n",
       "223993   1520281114     t5_2xtuc  t3_827pgt  t1_dv87o36     24  1.524846e+09   \n",
       "223994   1520313546     t5_2xtuc  t3_827pgt  t1_dv930hi     13  1.524864e+09   \n",
       "223995   1520288823     t5_2xtuc  t3_827pgt  t1_dv8go5a     10  1.524851e+09   \n",
       "223996   1520358619     t5_2xtuc  t3_827pgt  t1_dv9x79p      2  1.524880e+09   \n",
       "223997   1520276053     t5_2xtuc  t3_827pgt  t1_dv83f4u     20  1.524843e+09   \n",
       "223998   1520311737     t5_2xtuc  t3_827pgt  t1_dv91tdd      5  1.524863e+09   \n",
       "223999   1520289900     t5_2xtuc  t3_827pgt  t1_dv8ivmu      7  1.524852e+09   \n",
       "\n",
       "        controversiality  gilded       id subreddit  ups distinguished  \\\n",
       "223985                 0       0  dv82gtb   AskDocs  NaN           NaN   \n",
       "223986                 0       0  dv867dt   AskDocs  NaN           NaN   \n",
       "223987                 0       0  dv8zdsb   AskDocs  NaN           NaN   \n",
       "223988                 0       0  dv8hmg0   AskDocs  NaN           NaN   \n",
       "223989                 0       0  dv8hi3n   AskDocs  NaN           NaN   \n",
       "223990                 0       0  dv8v4ny   AskDocs  NaN           NaN   \n",
       "223991                 0       0  dv8h1e9   AskDocs  NaN           NaN   \n",
       "223992                 0       0  dv822m9   AskDocs  NaN           NaN   \n",
       "223993                 0       0  dv89g6u   AskDocs  NaN           NaN   \n",
       "223994                 0       0  dv94al4   AskDocs  NaN           NaN   \n",
       "223995                 0       0  dv8hwgy   AskDocs  NaN           NaN   \n",
       "223996                 0       0  dv9yhg6   AskDocs  NaN           NaN   \n",
       "223997                 0       0  dv840aq   AskDocs  NaN           NaN   \n",
       "223998                 0       0  dv930qn   AskDocs  NaN           NaN   \n",
       "223999                 0       0  dv8izqc   AskDocs  NaN           NaN   \n",
       "\n",
       "       author_flair_css_class  removal_reason       pd  \n",
       "223985                    NaN             NaN  dv829t2  \n",
       "223986                    NaN             NaN  dv8643f  \n",
       "223987                    NaN             NaN  dv8y923  \n",
       "223988                    NaN             NaN  dv8fd0y  \n",
       "223989                    NaN             NaN  dv8ed1z  \n",
       "223990                    NaN             NaN  dv8rpap  \n",
       "223991                    NaN             NaN  dv8fe0l  \n",
       "223992                    NaN             NaN  dv81ye0  \n",
       "223993                    NaN             NaN  dv87o36  \n",
       "223994                    NaN             NaN  dv930hi  \n",
       "223995                    NaN             NaN  dv8go5a  \n",
       "223996                    NaN             NaN  dv9x79p  \n",
       "223997                    NaN             NaN  dv83f4u  \n",
       "223998                    NaN             NaN  dv91tdd  \n",
       "223999                    NaN             NaN  dv8ivmu  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['author']=='kiwikellie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47080</th>\n",
       "      <td>I'm not a doctor, but do you guys think this i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yossi25</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1522308033</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.525707e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dwgoznj</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59878</th>\n",
       "      <td>Degenerative issues run in my family. From a y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vanteal</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520299064</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.524857e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8rpap</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154535</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520316050</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.524865e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv95z1b</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189543</th>\n",
       "      <td>Im so sorry for your loss. I’m not a doctor so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twinkie45</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520287643</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>6</td>\n",
       "      <td>1.524851e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8go5a</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195752</th>\n",
       "      <td>Sorry for your loss. It is never easy, she had...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DennyBenny</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520297740</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524856e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8qg82</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253902</th>\n",
       "      <td>It's not remotely the same but my father was a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hobbitlover</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520298169</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>6</td>\n",
       "      <td>1.524856e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8que6</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254645</th>\n",
       "      <td>She had degenerative disc and thinning disc fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jbjbjb55555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520310309</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.524862e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dv91xtu</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348588</th>\n",
       "      <td>Her fears of further degeneration and living i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MDFrankensteen</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274122</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>45</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv81ye0</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390103</th>\n",
       "      <td>that looks alot like my MRI. I also have ddd. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molotavcocktail</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520279487</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>18</td>\n",
       "      <td>1.524845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv87o36</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405258</th>\n",
       "      <td>So sorry for your loss.\\n\\nRoughly read throug...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sgmedicalstudent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520274423</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>81</td>\n",
       "      <td>1.524842e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv829t2</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444718</th>\n",
       "      <td>I hope that you find some peace, comfort, and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SerenityNOW_or_else_</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1520286966</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>t3_827pgt</td>\n",
       "      <td>7</td>\n",
       "      <td>1.524850e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dv8fxdl</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827pgt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body score_hidden  \\\n",
       "47080   I'm not a doctor, but do you guys think this i...          NaN   \n",
       "59878   Degenerative issues run in my family. From a y...          NaN   \n",
       "154535                                          [deleted]          NaN   \n",
       "189543  Im so sorry for your loss. I’m not a doctor so...          NaN   \n",
       "195752  Sorry for your loss. It is never easy, she had...          NaN   \n",
       "253902  It's not remotely the same but my father was a...          NaN   \n",
       "254645  She had degenerative disc and thinning disc fr...          NaN   \n",
       "348588  Her fears of further degeneration and living i...          NaN   \n",
       "390103  that looks alot like my MRI. I also have ddd. ...          NaN   \n",
       "405258  So sorry for your loss.\\n\\nRoughly read throug...          NaN   \n",
       "444718  I hope that you find some peace, comfort, and ...          NaN   \n",
       "\n",
       "       archived name                author  \\\n",
       "47080       NaN  NaN               Yossi25   \n",
       "59878       NaN  NaN               vanteal   \n",
       "154535      NaN  NaN             [deleted]   \n",
       "189543      NaN  NaN             twinkie45   \n",
       "195752      NaN  NaN            DennyBenny   \n",
       "253902      NaN  NaN           hobbitlover   \n",
       "254645      NaN  NaN           jbjbjb55555   \n",
       "348588      NaN  NaN        MDFrankensteen   \n",
       "390103      NaN  NaN       molotavcocktail   \n",
       "405258      NaN  NaN      Sgmedicalstudent   \n",
       "444718      NaN  NaN  SerenityNOW_or_else_   \n",
       "\n",
       "                           author_flair_text  downs  created_utc subreddit_id  \\\n",
       "47080   This user has not yet been verified.    NaN   1522308033     t5_2xtuc   \n",
       "59878   This user has not yet been verified.    NaN   1520299064     t5_2xtuc   \n",
       "154535                                   NaN    NaN   1520316050     t5_2xtuc   \n",
       "189543  This user has not yet been verified.    NaN   1520287643     t5_2xtuc   \n",
       "195752  This user has not yet been verified.    NaN   1520297740     t5_2xtuc   \n",
       "253902                                   NaN    NaN   1520298169     t5_2xtuc   \n",
       "254645                                   NaN    NaN   1520310309     t5_2xtuc   \n",
       "348588  This user has not yet been verified.    NaN   1520274122     t5_2xtuc   \n",
       "390103  This user has not yet been verified.    NaN   1520279487     t5_2xtuc   \n",
       "405258                                   NaN    NaN   1520274423     t5_2xtuc   \n",
       "444718  This user has not yet been verified.    NaN   1520286966     t5_2xtuc   \n",
       "\n",
       "          link_id  parent_id  score  retrieved_on  controversiality  gilded  \\\n",
       "47080   t3_827pgt  t3_827pgt      1  1.525707e+09                 0       0   \n",
       "59878   t3_827pgt  t3_827pgt      5  1.524857e+09                 0       0   \n",
       "154535  t3_827pgt  t3_827pgt      1  1.524865e+09                 0       0   \n",
       "189543  t3_827pgt  t3_827pgt      6  1.524851e+09                 0       0   \n",
       "195752  t3_827pgt  t3_827pgt      2  1.524856e+09                 0       0   \n",
       "253902  t3_827pgt  t3_827pgt      6  1.524856e+09                 0       0   \n",
       "254645  t3_827pgt  t3_827pgt     -2  1.524862e+09                 1       0   \n",
       "348588  t3_827pgt  t3_827pgt     45  1.524842e+09                 0       0   \n",
       "390103  t3_827pgt  t3_827pgt     18  1.524845e+09                 0       0   \n",
       "405258  t3_827pgt  t3_827pgt     81  1.524842e+09                 0       0   \n",
       "444718  t3_827pgt  t3_827pgt      7  1.524850e+09                 0       0   \n",
       "\n",
       "             id subreddit  ups distinguished author_flair_css_class  \\\n",
       "47080   dwgoznj   AskDocs  NaN           NaN                default   \n",
       "59878   dv8rpap   AskDocs  NaN           NaN                default   \n",
       "154535  dv95z1b   AskDocs  NaN           NaN                    NaN   \n",
       "189543  dv8go5a   AskDocs  NaN           NaN                default   \n",
       "195752  dv8qg82   AskDocs  NaN           NaN                default   \n",
       "253902  dv8que6   AskDocs  NaN           NaN                    NaN   \n",
       "254645  dv91xtu   AskDocs  NaN           NaN                    NaN   \n",
       "348588  dv81ye0   AskDocs  NaN           NaN                default   \n",
       "390103  dv87o36   AskDocs  NaN           NaN                default   \n",
       "405258  dv829t2   AskDocs  NaN           NaN                    NaN   \n",
       "444718  dv8fxdl   AskDocs  NaN           NaN                default   \n",
       "\n",
       "        removal_reason      pd  \n",
       "47080              NaN  827pgt  \n",
       "59878              NaN  827pgt  \n",
       "154535             NaN  827pgt  \n",
       "189543             NaN  827pgt  \n",
       "195752             NaN  827pgt  \n",
       "253902             NaN  827pgt  \n",
       "254645             NaN  827pgt  \n",
       "348588             NaN  827pgt  \n",
       "390103             NaN  827pgt  \n",
       "405258             NaN  827pgt  \n",
       "444718             NaN  827pgt  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.pd.str.contains('827pgt',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic types\n",
    "sem_type_dict = load_sem_types('../data/SemGroups_2013.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She's had it for 8 months, we've never had any issues with it before.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tagger.nlp(df['body'].iloc[4])\n",
    "t.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 19, 'end': 25, 'ngram': 'months', 'term': 'month', 'cui': 'C1561542', 'similarity': 0.75, 'semtypes': {'T170'}, 'preferred': 1}] \n",
      "\n",
      "[{'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C0033213', 'similarity': 0.75, 'semtypes': {'T033'}, 'preferred': 1}, {'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C1706387', 'similarity': 0.75, 'semtypes': {'T170'}, 'preferred': 1}] \n",
      "\n",
      "{'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C0033213', 'similarity': 0.75, 'semtypes': {'T033'}, 'preferred': 1}\n",
      "{'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C1706387', 'similarity': 0.75, 'semtypes': {'T170'}, 'preferred': 1}\n"
     ]
    }
   ],
   "source": [
    "t = tagger.nlp(t.text)\n",
    "\n",
    "s = t\n",
    "matches= tagger.match(s, best_match=True, ignore_syntax=False)\n",
    "for match in matches:\n",
    "    dir(match)\n",
    "    print(match,'\\n')\n",
    "for m in match:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-56b3b167801b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sem_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/SemGroups_2013.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'semtypes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "sd = load_sem_types('../data/SemGroups_2013.txt')\n",
    "m['semtypes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T170'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 19, 25, 'month', 'C1561542', 0.75, {'Intellectual Product'}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from processing import tag_utterances\n",
    "from processing import load_sem_types\n",
    "\n",
    "tag_utterances(1, t.text, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557648, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over every document\n",
      "Documents processed: 0\n",
      "Documents processed: 10000\n",
      "Documents processed: 20000\n",
      "Documents processed: 30000\n",
      "Documents processed: 40000\n",
      "Documents processed: 50000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 60000\n",
      "Documents processed: 70000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 80000\n",
      "Documents processed: 90000\n",
      "Documents processed: 100000\n",
      "Documents processed: 110000\n",
      "Documents processed: 120000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 130000\n",
      "Documents processed: 140000\n",
      "Documents processed: 150000\n",
      "Documents processed: 160000\n",
      "Documents processed: 170000\n",
      "Documents processed: 180000\n",
      "Documents processed: 190000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 200000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 210000\n",
      "Documents processed: 220000\n",
      "Documents processed: 230000\n",
      "Documents processed: 240000\n",
      "Documents processed: 250000\n",
      "Documents processed: 260000\n",
      "Documents processed: 270000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/Projects/Automated-Health-Responses/notebooks/processing.py\u001b[0m in \u001b[0;36mtag_utterances\u001b[0;34m(id, txt, tagger, sems)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstpwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Run the UMLS tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmatches\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_syntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/quickumls.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, text, best_match, ignore_syntax)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_all_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/quickumls.py\u001b[0m in \u001b[0;36m_get_all_matches\u001b[0;34m(self, ngrams)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mprev_cui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mngram_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mss_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mngram_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/toolbox.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, term)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_string_for_db_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/simstring/simstring.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0m__del__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Iterating over every document')\n",
    "#Iterate over every document and extract the concepts\n",
    "i=-1        \n",
    "result = []\n",
    "for idx,doc in  enumerate(df['body']):\n",
    "\n",
    "    if idx % 10000 == 0:\n",
    "        print(\"Documents processed: {}\".format(idx))\n",
    "    try:\n",
    "        i+=1\n",
    "        annotations = tag_utterances(i,doc,tagger)\n",
    "        result.extend(annotations)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "df_matches = pd.DataFrame(data=result, columns =['document','start','end','term','cui','similarity','semtypes'])\n",
    "df_matches.sort_values(by=['document','start'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>...</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for a manlet such as yourself I'd recommend at...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-Ai</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1513411674</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_7k5x2h</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514772e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>drbt2db</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you very much for answering!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-SY</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1445798103</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_3q697b</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.447190e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cwcfjpr</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never been tested for that.  I was hoping the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-o2</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1461952470</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_4gz1fi</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.463777e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d2mce34</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She said her constant abdominal pain is a 6, t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05P</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1504214332</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_6x9jk0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.504553e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dme9lzr</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She's had it for 8 months, we've never had any...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05P</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1504217835</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_6x9jk0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.504554e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dmecohs</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body score_hidden archived  \\\n",
       "0  for a manlet such as yourself I'd recommend at...          NaN      NaN   \n",
       "1                 Thank you very much for answering!          NaN      NaN   \n",
       "2  Never been tested for that.  I was hoping the ...          NaN      NaN   \n",
       "3  She said her constant abdominal pain is a 6, t...          NaN      NaN   \n",
       "4  She's had it for 8 months, we've never had any...          NaN      NaN   \n",
       "\n",
       "  name author                     author_flair_text  downs  created_utc  \\\n",
       "0  NaN    -Ai  This user has not yet been verified.    NaN   1513411674   \n",
       "1  NaN    -SY  This user has not yet been verified.    NaN   1445798103   \n",
       "2  NaN    -o2  This user has not yet been verified.    NaN   1461952470   \n",
       "3  NaN    05P  This user has not yet been verified.    NaN   1504214332   \n",
       "4  NaN    05P  This user has not yet been verified.    NaN   1504217835   \n",
       "\n",
       "  subreddit_id    link_id      ...       score  retrieved_on  \\\n",
       "0     t5_2xtuc  t3_7k5x2h      ...           0  1.514772e+09   \n",
       "1     t5_2xtuc  t3_3q697b      ...           2  1.447190e+09   \n",
       "2     t5_2xtuc  t3_4gz1fi      ...           1  1.463777e+09   \n",
       "3     t5_2xtuc  t3_6x9jk0      ...           1  1.504553e+09   \n",
       "4     t5_2xtuc  t3_6x9jk0      ...           1  1.504554e+09   \n",
       "\n",
       "   controversiality  gilded       id subreddit  ups  distinguished  \\\n",
       "0                 0       0  drbt2db   AskDocs  NaN            NaN   \n",
       "1                 0       0  cwcfjpr   AskDocs  2.0            NaN   \n",
       "2                 0       0  d2mce34   AskDocs  1.0            NaN   \n",
       "3                 0       0  dme9lzr   AskDocs  NaN            NaN   \n",
       "4                 0       0  dmecohs   AskDocs  NaN            NaN   \n",
       "\n",
       "  author_flair_css_class removal_reason  \n",
       "0                default            NaN  \n",
       "1                default            NaN  \n",
       "2                default            NaN  \n",
       "3                default            NaN  \n",
       "4                default            NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This user has not yet been verified.     352466\n",
       "Physician                                 26484\n",
       "Medical Student                           13399\n",
       "B.S., Medical Lab Sciences                 3069\n",
       "FY1 Doctor                                 2860\n",
       "Moderator                                  2644\n",
       "Registered Nurse                           2589\n",
       "Emergency Medical Technician               2023\n",
       "Surgeon                                    1765\n",
       "Surgeon | Moderator                        1710\n",
       "Orthopaedic Surgeon                        1569\n",
       "Psychiatrist                               1513\n",
       "Dermatologist                              1481\n",
       "Doctor (A&amp;E)                           1435\n",
       "Pharm.D. - Hospital Pharmacist             1056\n",
       "Emergency Physician                        1027\n",
       "Nursing Graduate, RPN                       803\n",
       "Pharm.D.                                    748\n",
       "Nursing Student                             735\n",
       "Occupational Therapist                      727\n",
       "Anesthesiologist                            659\n",
       "Doctor                                      632\n",
       "Clinical Counselor                          599\n",
       "Physician Assistant Student                 572\n",
       "Emergency Medicine Physician                564\n",
       "Physician, Pediatrics                       523\n",
       "Midwifery Student                           523\n",
       "Physician Assistant                         509\n",
       "Physician | Moderator                       496\n",
       "Physician/Neurosurgeon                      493\n",
       "                                          ...  \n",
       "Registered Nurse                             14\n",
       "Genetics Counselor                           14\n",
       "Paramedic student                            13\n",
       "Clinical Psychologist                        12\n",
       "Histopathologist                             11\n",
       "Ob/gyn MD                                    11\n",
       "WEB DEVELOPER                                 9\n",
       "Student Radiographer                          8\n",
       "PhD Candidate (Oral Microbiology)             8\n",
       "Epidemiologist                                7\n",
       "Web Developer                                 7\n",
       "Medical Intern                                7\n",
       "Nurse Practitioner, Internal Medicine         6\n",
       "Medical Student`                              6\n",
       "Orthopedic Surgeon                            5\n",
       "Physician and Surgeon                         4\n",
       "EMT, BSN Student                              4\n",
       "/r/Health                                     3\n",
       "mental health worker                          3\n",
       "Lead Web Developer                            2\n",
       "M.D. - Pediatrician                           2\n",
       "Optometrist                                   2\n",
       "Genetic Counselor                             2\n",
       "Medical Scientist                             2\n",
       "Geriatrician                                  1\n",
       "Cardiologist                                  1\n",
       "Chiropractor/RN                               1\n",
       "National Guard Medic                          1\n",
       "Imaging Technician                            1\n",
       "Physician, Neurosurgeon/Moderator             1\n",
       "Name: author_flair_text, Length: 159, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author_flair_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Clinician"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36794178406449946\n",
      "0    352466\n",
      "1    205182\n",
      "Name: is_clinician, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['is_clinician'] = df['author_flair_text'].apply(lambda r: 0 if r =='This user has not yet been verified.' else 1)\n",
    "print(df['is_clinician'].mean())\n",
    "print(df['is_clinician'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "df['tokenized_sents'] = df['body'].apply(lambda row: str(row).strip().replace('\\n','').lower().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body']  = df['body'].apply(lambda r: str(r))\n",
    "df['body'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e87deb804f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_sents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-e87deb804f23>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_sents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0;31m# internal: pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['tokenized_sents'] = df['body'].apply(lambda row: nltk.word_tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tokenized_sents'].tolist()\n",
    "# Remove empty strings\n",
    "X = [[t for t in sent if len(t)>1] for sent in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"../data/glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(X, size=100)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got ourselves a dictionary mapping word -> 100-dimensional vector. Now we can use it to build features. The simplest way to do that is by averaging word vectors for all words in a text. We will build a sklearn-compatible transformer that is initialised with a word -> vector dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    \"\"\"\n",
    "    Average word vectors for all words in a text using n dimensional word embedding vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # If a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    # If text is empty, return vector of zeros\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s throw in a version that uses tf-idf weighting scheme for good measurem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0f2863e786bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m etree_w2v = Pipeline([\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=7))])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=7))])\n",
    "\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "etree_w2v.fit(X,df['is_clinician'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(lowercase=False,ngram_range=(1,2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,df['is_clinician'] , test_size=0.2, random_state=329)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibTypeError",
     "evalue": "JoblibTypeError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x10458e8a0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/austinpowell/Google_Drive/kp_datascience/...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/austinpowell/Google_Drive/kp_datascience/...lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/austi.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x10458e8a0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/austinpowell/Google_Drive/kp_datascience/...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/austinpowell/Google_Drive/kp_datascience/...lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/austi.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    417             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    418                                    finalizer=self._asyncgen_finalizer_hook)\n    419         try:\n    420             events._set_running_loop(self)\n    421             while True:\n--> 422                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    423                 if self._stopping:\n    424                     break\n    425         finally:\n    426             self._stopping = False\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1427                         logger.warning('Executing %s took %.3f seconds',\n   1428                                        _format_handle(handle), dt)\n   1429                 finally:\n   1430                     self._current_handle = None\n   1431             else:\n-> 1432                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(14, 1)>>\n   1433         handle = None  # Needed to break cycles when an exception occurs.\n   1434 \n   1435     def _set_coroutine_wrapper(self, enabled):\n   1436         try:\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(14, 1)>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (14, 1)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=14, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 26, 22, 48, 42, 639541, tzinfo=tzutc()), 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'session': '68a5e41f0cea459617cafa0dc26051f1', 'username': '', 'version': '5.2'}, 'metadata': {}, 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'68a5e41f0cea459617cafa0dc26051f1']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 26, 22, 48, 42, 639541, tzinfo=tzutc()), 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'session': '68a5e41f0cea459617cafa0dc26051f1', 'username': '', 'version': '5.2'}, 'metadata': {}, 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'68a5e41f0cea459617cafa0dc26051f1'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 26, 22, 48, 42, 639541, tzinfo=tzutc()), 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'session': '68a5e41f0cea459617cafa0dc26051f1', 'username': '', 'version': '5.2'}, 'metadata': {}, 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\"\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\",), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\",)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-34-8df885792402>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1d9061e10, execution_..._before_exec=None error_in_exec=None result=None>)\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n   2855                 code = compiler(mod, cell_name, \"single\")\n-> 2856                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x1d9060c00, file \"<ipython-input-34-8df885792402>\", line 1>\n        result = <ExecutionResult object at 1d9061e10, execution_..._before_exec=None error_in_exec=None result=None>\n   2857                     return True\n   2858 \n   2859             # Flush softspace\n   2860             if softspace(sys.stdout, 0):\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x1d9060c00, file \"<ipython-input-34-8df885792402>\", line 1>, result=<ExecutionResult object at 1d9061e10, execution_..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x1d9060c00, file \"<ipython-input-34-8df885792402>\", line 1>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', '# Utility\\nimport sys,os\\nimport time\\n\\nimport pand... QuickUMLS\\ntagger = QuickUMLS(abs_path_data_umls)', \"path_to_data = '../data/reddit_comments_askDocs_..._memory=False)\\nprint('Shape',df.shape)\\ndf.head(2)\", \"# Load semantic types\\nsem_type_dict = load_sem_types('../data/SemGroups_2013.txt')\", 'from sklearn.feature_extraction.text import Tfid...r\\nfrom collections import defaultdict\\nimport time', \"df['is_clinician'] = df['author_flair_text'].app....mean())\\nprint(df['is_clinician'].value_counts())\", \"import pandas as pd\\nimport nltk\\n\\ndf['tokenized_s...row).strip().replace('\\\\n','').lower().split(' '))\", \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', 'class MeanEmbeddingVectorizer(object):\\n    \"\"\"\\n ...)], axis=0)\\n            for words in X\\n        ])', 'class TfidfEmbeddingVectorizer(object):\\n    def ...=0)\\n                for words in X\\n            ])', 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", \"from sklearn.model_selection import cross_val_sc...lf, X_train, y_train, cv=5,n_jobs=7,scoring='f1')\", \"X = df['tokenized_sents'].tolist()\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s..., X_train, y_train, cv=5,n_jobs=7,scoring=\\'f1\\')\")', 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s...ain, y_train, cv=5,n_jobs=7,scoring=\\'f1_macro\\')\")', 'print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))', ...], 'MeanEmbeddingVectorizer': <class '__main__.MeanEmbeddingVectorizer'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {2:                                                 ...  default            NaN  \n\n[2 rows x 21 columns], 25: [['for', 'a', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', '7', 'hours', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', '', 'i', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'a', 'lot', 'of', 'things.', '', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', 'a', '6,', 'the', 'pain', 'during', 'sex', 'was', 'a', '9', '(we', 'had', 'to', 'stop', ...]], 29: 0, 31: [['for', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', 'hours', 'of', 'sleep.', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'lot', 'of', 'things.', 'am', 'gonna', 'find', 'good', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', '6,', 'the', 'pain', 'during', 'sex', 'was', '(we', 'had', 'to', 'stop', 'immediately).', 'no', 'fever.'], [\"she's\", 'had', 'it', 'for', 'months,', \"we've\", 'never', 'had', 'any', 'issues', 'with', 'it', 'before.'], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], [\"here's\", 'the', 'full', 'enchilada:dimensions/calcs', 'bsa', 'adjustedlvidd', 'm-mode', '5.7', 'cm', '(3.5-5.7)', 'lvidd', 'm-mode', '2.89', '(1.9-3.2)', 'lvidd', '2d', '4.83', 'cm', '(3.5-5.7)', 'lvidd', ...], ['the', 'attending', 'at', 'the', 'hospital', 'said', 'they', 'wanted', 'to', 'do', 'follow-up', 'ekg.', 'was', 'hesitant', 'because', 'my', 'insurance', 'sucks', 'and', \"it's\", ...], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], ['laying', 'here', 'due', 'to', 'severe', 'sciatica', 'and', 'meralgia', 'paresthetica.', 'seriously', 'sympathize', 'and', 'empathize', 'your', 'pain.', 'bless', 'you.']]}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'QuickUMLS': <class 'quickumls.QuickUMLS'>, 'TfidfEmbeddingVectorizer': <class '__main__.TfidfEmbeddingVectorizer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', '# Utility\\nimport sys,os\\nimport time\\n\\nimport pand... QuickUMLS\\ntagger = QuickUMLS(abs_path_data_umls)', \"path_to_data = '../data/reddit_comments_askDocs_..._memory=False)\\nprint('Shape',df.shape)\\ndf.head(2)\", \"# Load semantic types\\nsem_type_dict = load_sem_types('../data/SemGroups_2013.txt')\", 'from sklearn.feature_extraction.text import Tfid...r\\nfrom collections import defaultdict\\nimport time', \"df['is_clinician'] = df['author_flair_text'].app....mean())\\nprint(df['is_clinician'].value_counts())\", \"import pandas as pd\\nimport nltk\\n\\ndf['tokenized_s...row).strip().replace('\\\\n','').lower().split(' '))\", \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', 'class MeanEmbeddingVectorizer(object):\\n    \"\"\"\\n ...)], axis=0)\\n            for words in X\\n        ])', 'class TfidfEmbeddingVectorizer(object):\\n    def ...=0)\\n                for words in X\\n            ])', 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", \"from sklearn.model_selection import cross_val_sc...lf, X_train, y_train, cv=5,n_jobs=7,scoring='f1')\", \"X = df['tokenized_sents'].tolist()\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s..., X_train, y_train, cv=5,n_jobs=7,scoring=\\'f1\\')\")', 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s...ain, y_train, cv=5,n_jobs=7,scoring=\\'f1_macro\\')\")', 'print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))', ...], 'MeanEmbeddingVectorizer': <class '__main__.MeanEmbeddingVectorizer'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {2:                                                 ...  default            NaN  \n\n[2 rows x 21 columns], 25: [['for', 'a', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', '7', 'hours', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', '', 'i', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'a', 'lot', 'of', 'things.', '', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', 'a', '6,', 'the', 'pain', 'during', 'sex', 'was', 'a', '9', '(we', 'had', 'to', 'stop', ...]], 29: 0, 31: [['for', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', 'hours', 'of', 'sleep.', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'lot', 'of', 'things.', 'am', 'gonna', 'find', 'good', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', '6,', 'the', 'pain', 'during', 'sex', 'was', '(we', 'had', 'to', 'stop', 'immediately).', 'no', 'fever.'], [\"she's\", 'had', 'it', 'for', 'months,', \"we've\", 'never', 'had', 'any', 'issues', 'with', 'it', 'before.'], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], [\"here's\", 'the', 'full', 'enchilada:dimensions/calcs', 'bsa', 'adjustedlvidd', 'm-mode', '5.7', 'cm', '(3.5-5.7)', 'lvidd', 'm-mode', '2.89', '(1.9-3.2)', 'lvidd', '2d', '4.83', 'cm', '(3.5-5.7)', 'lvidd', ...], ['the', 'attending', 'at', 'the', 'hospital', 'said', 'they', 'wanted', 'to', 'do', 'follow-up', 'ekg.', 'was', 'hesitant', 'because', 'my', 'insurance', 'sucks', 'and', \"it's\", ...], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], ['laying', 'here', 'due', 'to', 'severe', 'sciatica', 'and', 'meralgia', 'paresthetica.', 'seriously', 'sympathize', 'and', 'empathize', 'your', 'pain.', 'bless', 'you.']]}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'QuickUMLS': <class 'quickumls.QuickUMLS'>, 'TfidfEmbeddingVectorizer': <class '__main__.TfidfEmbeddingVectorizer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/Projects/Automated-Health-Responses/notebooks/<ipython-input-34-8df885792402> in <module>()\n----> 1 get_ipython().run_cell_magic('time', '', \"from sklearn.model_selection import cross_val_score\\n\\nscores = cross_val_score(text_clf, X_train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\")\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, magic_name='time', line='', cell=\"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\")\n   2126             # This will need to be updated if the internal calling logic gets\n   2127             # refactored, or else we'll be expanding the wrong variables.\n   2128             stack_depth = 2\n   2129             magic_arg_s = self.var_expand(line, stack_depth)\n   2130             with self.builtin_trap:\n-> 2131                 result = fn(magic_arg_s, cell)\n        result = undefined\n        fn = <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object>>\n        magic_arg_s = ''\n        cell = \"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\"\n   2132             return result\n   2133 \n   2134     def find_line_magic(self, magic_name):\n   2135         \"\"\"Find and return a line magic by name.\n\n...........................................................................\n/Users/austinpowell/Google_Drive/Projects/Automated-Health-Responses/notebooks/<decorator-gen-62> in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", local_ns=None)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/magic.py in <lambda>(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, '', \"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", None), **k={})\n    182     validate_type(magic_kind)\n    183 \n    184     # This is a closure to capture the magic_kind.  We could also use a class,\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--> 187         call = lambda f, *a, **k: f(*a, **k)\n        f = <function ExecutionMagics.time>\n        a = (<IPython.core.magics.execution.ExecutionMagics object>, '', \"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", None)\n        k = {}\n    188 \n    189         if callable(arg):\n    190             # \"Naked\" decorator call (just @foo, no args)\n    191             func = arg\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/magics/execution.py in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", local_ns=None)\n   1233                 return\n   1234             end = clock2()\n   1235         else:\n   1236             st = clock2()\n   1237             try:\n-> 1238                 exec(code, glob, local_ns)\n        code = <code object <module> at 0x180ade1e0, file \"<timed exec>\", line 1>\n        glob = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', '# Utility\\nimport sys,os\\nimport time\\n\\nimport pand... QuickUMLS\\ntagger = QuickUMLS(abs_path_data_umls)', \"path_to_data = '../data/reddit_comments_askDocs_..._memory=False)\\nprint('Shape',df.shape)\\ndf.head(2)\", \"# Load semantic types\\nsem_type_dict = load_sem_types('../data/SemGroups_2013.txt')\", 'from sklearn.feature_extraction.text import Tfid...r\\nfrom collections import defaultdict\\nimport time', \"df['is_clinician'] = df['author_flair_text'].app....mean())\\nprint(df['is_clinician'].value_counts())\", \"import pandas as pd\\nimport nltk\\n\\ndf['tokenized_s...row).strip().replace('\\\\n','').lower().split(' '))\", \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', 'class MeanEmbeddingVectorizer(object):\\n    \"\"\"\\n ...)], axis=0)\\n            for words in X\\n        ])', 'class TfidfEmbeddingVectorizer(object):\\n    def ...=0)\\n                for words in X\\n            ])', 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", \"from sklearn.model_selection import cross_val_sc...lf, X_train, y_train, cv=5,n_jobs=7,scoring='f1')\", \"X = df['tokenized_sents'].tolist()\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s..., X_train, y_train, cv=5,n_jobs=7,scoring=\\'f1\\')\")', 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s...ain, y_train, cv=5,n_jobs=7,scoring=\\'f1_macro\\')\")', 'print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))', ...], 'MeanEmbeddingVectorizer': <class '__main__.MeanEmbeddingVectorizer'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {2:                                                 ...  default            NaN  \n\n[2 rows x 21 columns], 25: [['for', 'a', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', '7', 'hours', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', '', 'i', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'a', 'lot', 'of', 'things.', '', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', 'a', '6,', 'the', 'pain', 'during', 'sex', 'was', 'a', '9', '(we', 'had', 'to', 'stop', ...]], 29: 0, 31: [['for', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', 'hours', 'of', 'sleep.', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'lot', 'of', 'things.', 'am', 'gonna', 'find', 'good', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', '6,', 'the', 'pain', 'during', 'sex', 'was', '(we', 'had', 'to', 'stop', 'immediately).', 'no', 'fever.'], [\"she's\", 'had', 'it', 'for', 'months,', \"we've\", 'never', 'had', 'any', 'issues', 'with', 'it', 'before.'], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], [\"here's\", 'the', 'full', 'enchilada:dimensions/calcs', 'bsa', 'adjustedlvidd', 'm-mode', '5.7', 'cm', '(3.5-5.7)', 'lvidd', 'm-mode', '2.89', '(1.9-3.2)', 'lvidd', '2d', '4.83', 'cm', '(3.5-5.7)', 'lvidd', ...], ['the', 'attending', 'at', 'the', 'hospital', 'said', 'they', 'wanted', 'to', 'do', 'follow-up', 'ekg.', 'was', 'hesitant', 'because', 'my', 'insurance', 'sucks', 'and', \"it's\", ...], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], ['laying', 'here', 'due', 'to', 'severe', 'sciatica', 'and', 'meralgia', 'paresthetica.', 'seriously', 'sympathize', 'and', 'empathize', 'your', 'pain.', 'bless', 'you.']]}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'QuickUMLS': <class 'quickumls.QuickUMLS'>, 'TfidfEmbeddingVectorizer': <class '__main__.TfidfEmbeddingVectorizer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n        local_ns = None\n   1239             except:\n   1240                 self.shell.showtraceback()\n   1241                 return\n   1242             end = clock2()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/Projects/Automated-Health-Responses/notebooks/<timed exec> in <module>()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, groups=None, scoring='f1_macro', cv=5, n_jobs=7, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, groups=None, scoring={'score': make_scorer(f1_score, pos_label=None, average=macro)}, cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=7, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=5, random_state=None, shuffle=False)>\n        X = [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...]\n        y = 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=7), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=7)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nTypeError                                          Sat May 26 15:49:09 2018\nPID: 48691Python 3.6.5: /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, scorer={'score': make_scorer(f1_score, pos_label=None, average=macro)}, train=memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), test=array([    0,     1,     2, ..., 89313, 89314, 89321]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y_train = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x1d8f811e0>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], fixed_vocab=False)\n    787         indptr = _make_int_array()\n    788         values = _make_int_array()\n    789         indptr.append(0)\n    790         for doc in raw_documents:\n    791             feature_counter = {}\n--> 792             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    793                 try:\n    794                     feature_idx = vocabulary[feature]\n    795                     if feature_idx not in feature_counter:\n    796                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    261         elif self.analyzer == 'word':\n    262             stop_words = self.get_stop_words()\n    263             tokenize = self.build_tokenizer()\n    264 \n    265             return lambda doc: self._word_ngrams(\n--> 266                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    267 \n    268         else:\n    269             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    270                              self.analyzer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    236     def build_tokenizer(self):\n    237         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n    238         if self.tokenizer is not None:\n    239             return self.tokenizer\n    240         token_pattern = re.compile(self.token_pattern)\n--> 241         return lambda doc: token_pattern.findall(doc)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    242 \n    243     def get_stop_words(self):\n    244         \"\"\"Build or fetch the effective stop words list\"\"\"\n    245         return _check_stop_list(self.stop_words)\n\nTypeError: expected string or bytes-like object\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py\", line 248, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py\", line 213, in _fit\n    **fit_params_steps[name])\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\", line 362, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py\", line 581, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 869, in fit_transform\n    self.fixed_vocabulary_)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 792, in _count_vocab\n    for feature in analyze(doc):\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 266, in <lambda>\n    tokenize(preprocess(self.decode(doc))), stop_words)\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 241, in <lambda>\n    return lambda doc: token_pattern.findall(doc)\nTypeError: expected string or bytes-like object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nTypeError                                          Sat May 26 15:49:09 2018\nPID: 48691Python 3.6.5: /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, scorer={'score': make_scorer(f1_score, pos_label=None, average=macro)}, train=memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), test=array([    0,     1,     2, ..., 89313, 89314, 89321]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y_train = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x1d8f811e0>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], fixed_vocab=False)\n    787         indptr = _make_int_array()\n    788         values = _make_int_array()\n    789         indptr.append(0)\n    790         for doc in raw_documents:\n    791             feature_counter = {}\n--> 792             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    793                 try:\n    794                     feature_idx = vocabulary[feature]\n    795                     if feature_idx not in feature_counter:\n    796                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    261         elif self.analyzer == 'word':\n    262             stop_words = self.get_stop_words()\n    263             tokenize = self.build_tokenizer()\n    264 \n    265             return lambda doc: self._word_ngrams(\n--> 266                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    267 \n    268         else:\n    269             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    270                              self.analyzer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    236     def build_tokenizer(self):\n    237         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n    238         if self.tokenizer is not None:\n    239             return self.tokenizer\n    240         token_pattern = re.compile(self.token_pattern)\n--> 241         return lambda doc: token_pattern.findall(doc)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    242 \n    243     def get_stop_words(self):\n    244         \"\"\"Build or fetch the effective stop words list\"\"\"\n    245         return _check_stop_list(self.stop_words)\n\nTypeError: expected string or bytes-like object\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nTypeError                                          Sat May 26 15:49:09 2018\nPID: 48691Python 3.6.5: /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, scorer={'score': make_scorer(f1_score, pos_label=None, average=macro)}, train=memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), test=array([    0,     1,     2, ..., 89313, 89314, 89321]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y_train = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x1d8f811e0>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], fixed_vocab=False)\n    787         indptr = _make_int_array()\n    788         values = _make_int_array()\n    789         indptr.append(0)\n    790         for doc in raw_documents:\n    791             feature_counter = {}\n--> 792             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    793                 try:\n    794                     feature_idx = vocabulary[feature]\n    795                     if feature_idx not in feature_counter:\n    796                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    261         elif self.analyzer == 'word':\n    262             stop_words = self.get_stop_words()\n    263             tokenize = self.build_tokenizer()\n    264 \n    265             return lambda doc: self._word_ngrams(\n--> 266                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    267 \n    268         else:\n    269             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    270                              self.analyzer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    236     def build_tokenizer(self):\n    237         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n    238         if self.tokenizer is not None:\n    239             return self.tokenizer\n    240         token_pattern = re.compile(self.token_pattern)\n--> 241         return lambda doc: token_pattern.findall(doc)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    242 \n    243     def get_stop_words(self):\n    244         \"\"\"Build or fetch the effective stop words list\"\"\"\n    245         return _check_stop_list(self.stop_words)\n\nTypeError: expected string or bytes-like object\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibTypeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibTypeError\u001b[0m: JoblibTypeError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x10458e8a0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/austinpowell/Google_Drive/kp_datascience/...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/austinpowell/Google_Drive/kp_datascience/...lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/austi.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x10458e8a0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/austinpowell/Google_Drive/kp_datascience/...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/austinpowell/Google_Drive/kp_datascience/...lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/austi.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    417             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    418                                    finalizer=self._asyncgen_finalizer_hook)\n    419         try:\n    420             events._set_running_loop(self)\n    421             while True:\n--> 422                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    423                 if self._stopping:\n    424                     break\n    425         finally:\n    426             self._stopping = False\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1427                         logger.warning('Executing %s took %.3f seconds',\n   1428                                        _format_handle(handle), dt)\n   1429                 finally:\n   1430                     self._current_handle = None\n   1431             else:\n-> 1432                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(14, 1)>>\n   1433         handle = None  # Needed to break cycles when an exception occurs.\n   1434 \n   1435     def _set_coroutine_wrapper(self, enabled):\n   1436         try:\n\n...........................................................................\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(14, 1)>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (14, 1)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=14, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 26, 22, 48, 42, 639541, tzinfo=tzutc()), 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'session': '68a5e41f0cea459617cafa0dc26051f1', 'username': '', 'version': '5.2'}, 'metadata': {}, 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'68a5e41f0cea459617cafa0dc26051f1']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 26, 22, 48, 42, 639541, tzinfo=tzutc()), 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'session': '68a5e41f0cea459617cafa0dc26051f1', 'username': '', 'version': '5.2'}, 'metadata': {}, 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'68a5e41f0cea459617cafa0dc26051f1'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 26, 22, 48, 42, 639541, tzinfo=tzutc()), 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'session': '68a5e41f0cea459617cafa0dc26051f1', 'username': '', 'version': '5.2'}, 'metadata': {}, 'msg_id': '1dbb08ea41b07df8b1d391f2bff6eade', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\"\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\",), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\",)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"%%time\\nfrom sklearn.model_selection import cross...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-34-8df885792402>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1d9061e10, execution_..._before_exec=None error_in_exec=None result=None>)\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n   2855                 code = compiler(mod, cell_name, \"single\")\n-> 2856                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x1d9060c00, file \"<ipython-input-34-8df885792402>\", line 1>\n        result = <ExecutionResult object at 1d9061e10, execution_..._before_exec=None error_in_exec=None result=None>\n   2857                     return True\n   2858 \n   2859             # Flush softspace\n   2860             if softspace(sys.stdout, 0):\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x1d9060c00, file \"<ipython-input-34-8df885792402>\", line 1>, result=<ExecutionResult object at 1d9061e10, execution_..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x1d9060c00, file \"<ipython-input-34-8df885792402>\", line 1>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', '# Utility\\nimport sys,os\\nimport time\\n\\nimport pand... QuickUMLS\\ntagger = QuickUMLS(abs_path_data_umls)', \"path_to_data = '../data/reddit_comments_askDocs_..._memory=False)\\nprint('Shape',df.shape)\\ndf.head(2)\", \"# Load semantic types\\nsem_type_dict = load_sem_types('../data/SemGroups_2013.txt')\", 'from sklearn.feature_extraction.text import Tfid...r\\nfrom collections import defaultdict\\nimport time', \"df['is_clinician'] = df['author_flair_text'].app....mean())\\nprint(df['is_clinician'].value_counts())\", \"import pandas as pd\\nimport nltk\\n\\ndf['tokenized_s...row).strip().replace('\\\\n','').lower().split(' '))\", \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', 'class MeanEmbeddingVectorizer(object):\\n    \"\"\"\\n ...)], axis=0)\\n            for words in X\\n        ])', 'class TfidfEmbeddingVectorizer(object):\\n    def ...=0)\\n                for words in X\\n            ])', 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", \"from sklearn.model_selection import cross_val_sc...lf, X_train, y_train, cv=5,n_jobs=7,scoring='f1')\", \"X = df['tokenized_sents'].tolist()\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s..., X_train, y_train, cv=5,n_jobs=7,scoring=\\'f1\\')\")', 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s...ain, y_train, cv=5,n_jobs=7,scoring=\\'f1_macro\\')\")', 'print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))', ...], 'MeanEmbeddingVectorizer': <class '__main__.MeanEmbeddingVectorizer'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {2:                                                 ...  default            NaN  \n\n[2 rows x 21 columns], 25: [['for', 'a', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', '7', 'hours', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', '', 'i', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'a', 'lot', 'of', 'things.', '', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', 'a', '6,', 'the', 'pain', 'during', 'sex', 'was', 'a', '9', '(we', 'had', 'to', 'stop', ...]], 29: 0, 31: [['for', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', 'hours', 'of', 'sleep.', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'lot', 'of', 'things.', 'am', 'gonna', 'find', 'good', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', '6,', 'the', 'pain', 'during', 'sex', 'was', '(we', 'had', 'to', 'stop', 'immediately).', 'no', 'fever.'], [\"she's\", 'had', 'it', 'for', 'months,', \"we've\", 'never', 'had', 'any', 'issues', 'with', 'it', 'before.'], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], [\"here's\", 'the', 'full', 'enchilada:dimensions/calcs', 'bsa', 'adjustedlvidd', 'm-mode', '5.7', 'cm', '(3.5-5.7)', 'lvidd', 'm-mode', '2.89', '(1.9-3.2)', 'lvidd', '2d', '4.83', 'cm', '(3.5-5.7)', 'lvidd', ...], ['the', 'attending', 'at', 'the', 'hospital', 'said', 'they', 'wanted', 'to', 'do', 'follow-up', 'ekg.', 'was', 'hesitant', 'because', 'my', 'insurance', 'sucks', 'and', \"it's\", ...], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], ['laying', 'here', 'due', 'to', 'severe', 'sciatica', 'and', 'meralgia', 'paresthetica.', 'seriously', 'sympathize', 'and', 'empathize', 'your', 'pain.', 'bless', 'you.']]}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'QuickUMLS': <class 'quickumls.QuickUMLS'>, 'TfidfEmbeddingVectorizer': <class '__main__.TfidfEmbeddingVectorizer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', '# Utility\\nimport sys,os\\nimport time\\n\\nimport pand... QuickUMLS\\ntagger = QuickUMLS(abs_path_data_umls)', \"path_to_data = '../data/reddit_comments_askDocs_..._memory=False)\\nprint('Shape',df.shape)\\ndf.head(2)\", \"# Load semantic types\\nsem_type_dict = load_sem_types('../data/SemGroups_2013.txt')\", 'from sklearn.feature_extraction.text import Tfid...r\\nfrom collections import defaultdict\\nimport time', \"df['is_clinician'] = df['author_flair_text'].app....mean())\\nprint(df['is_clinician'].value_counts())\", \"import pandas as pd\\nimport nltk\\n\\ndf['tokenized_s...row).strip().replace('\\\\n','').lower().split(' '))\", \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', 'class MeanEmbeddingVectorizer(object):\\n    \"\"\"\\n ...)], axis=0)\\n            for words in X\\n        ])', 'class TfidfEmbeddingVectorizer(object):\\n    def ...=0)\\n                for words in X\\n            ])', 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", \"from sklearn.model_selection import cross_val_sc...lf, X_train, y_train, cv=5,n_jobs=7,scoring='f1')\", \"X = df['tokenized_sents'].tolist()\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s..., X_train, y_train, cv=5,n_jobs=7,scoring=\\'f1\\')\")', 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s...ain, y_train, cv=5,n_jobs=7,scoring=\\'f1_macro\\')\")', 'print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))', ...], 'MeanEmbeddingVectorizer': <class '__main__.MeanEmbeddingVectorizer'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {2:                                                 ...  default            NaN  \n\n[2 rows x 21 columns], 25: [['for', 'a', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', '7', 'hours', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', '', 'i', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'a', 'lot', 'of', 'things.', '', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', 'a', '6,', 'the', 'pain', 'during', 'sex', 'was', 'a', '9', '(we', 'had', 'to', 'stop', ...]], 29: 0, 31: [['for', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', 'hours', 'of', 'sleep.', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'lot', 'of', 'things.', 'am', 'gonna', 'find', 'good', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', '6,', 'the', 'pain', 'during', 'sex', 'was', '(we', 'had', 'to', 'stop', 'immediately).', 'no', 'fever.'], [\"she's\", 'had', 'it', 'for', 'months,', \"we've\", 'never', 'had', 'any', 'issues', 'with', 'it', 'before.'], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], [\"here's\", 'the', 'full', 'enchilada:dimensions/calcs', 'bsa', 'adjustedlvidd', 'm-mode', '5.7', 'cm', '(3.5-5.7)', 'lvidd', 'm-mode', '2.89', '(1.9-3.2)', 'lvidd', '2d', '4.83', 'cm', '(3.5-5.7)', 'lvidd', ...], ['the', 'attending', 'at', 'the', 'hospital', 'said', 'they', 'wanted', 'to', 'do', 'follow-up', 'ekg.', 'was', 'hesitant', 'because', 'my', 'insurance', 'sucks', 'and', \"it's\", ...], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], ['laying', 'here', 'due', 'to', 'severe', 'sciatica', 'and', 'meralgia', 'paresthetica.', 'seriously', 'sympathize', 'and', 'empathize', 'your', 'pain.', 'bless', 'you.']]}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'QuickUMLS': <class 'quickumls.QuickUMLS'>, 'TfidfEmbeddingVectorizer': <class '__main__.TfidfEmbeddingVectorizer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/Projects/Automated-Health-Responses/notebooks/<ipython-input-34-8df885792402> in <module>()\n----> 1 get_ipython().run_cell_magic('time', '', \"from sklearn.model_selection import cross_val_score\\n\\nscores = cross_val_score(text_clf, X_train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\")\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, magic_name='time', line='', cell=\"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\")\n   2126             # This will need to be updated if the internal calling logic gets\n   2127             # refactored, or else we'll be expanding the wrong variables.\n   2128             stack_depth = 2\n   2129             magic_arg_s = self.var_expand(line, stack_depth)\n   2130             with self.builtin_trap:\n-> 2131                 result = fn(magic_arg_s, cell)\n        result = undefined\n        fn = <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object>>\n        magic_arg_s = ''\n        cell = \"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\"\n   2132             return result\n   2133 \n   2134     def find_line_magic(self, magic_name):\n   2135         \"\"\"Find and return a line magic by name.\n\n...........................................................................\n/Users/austinpowell/Google_Drive/Projects/Automated-Health-Responses/notebooks/<decorator-gen-62> in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", local_ns=None)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/magic.py in <lambda>(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, '', \"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", None), **k={})\n    182     validate_type(magic_kind)\n    183 \n    184     # This is a closure to capture the magic_kind.  We could also use a class,\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--> 187         call = lambda f, *a, **k: f(*a, **k)\n        f = <function ExecutionMagics.time>\n        a = (<IPython.core.magics.execution.ExecutionMagics object>, '', \"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", None)\n        k = {}\n    188 \n    189         if callable(arg):\n    190             # \"Naked\" decorator call (just @foo, no args)\n    191             func = arg\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/magics/execution.py in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell=\"from sklearn.model_selection import cross_val_sc...train, y_train, cv=5,n_jobs=7,scoring='f1_macro')\", local_ns=None)\n   1233                 return\n   1234             end = clock2()\n   1235         else:\n   1236             st = clock2()\n   1237             try:\n-> 1238                 exec(code, glob, local_ns)\n        code = <code object <module> at 0x180ade1e0, file \"<timed exec>\", line 1>\n        glob = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', '# Utility\\nimport sys,os\\nimport time\\n\\nimport pand... QuickUMLS\\ntagger = QuickUMLS(abs_path_data_umls)', \"path_to_data = '../data/reddit_comments_askDocs_..._memory=False)\\nprint('Shape',df.shape)\\ndf.head(2)\", \"# Load semantic types\\nsem_type_dict = load_sem_types('../data/SemGroups_2013.txt')\", 'from sklearn.feature_extraction.text import Tfid...r\\nfrom collections import defaultdict\\nimport time', \"df['is_clinician'] = df['author_flair_text'].app....mean())\\nprint(df['is_clinician'].value_counts())\", \"import pandas as pd\\nimport nltk\\n\\ndf['tokenized_s...row).strip().replace('\\\\n','').lower().split(' '))\", \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', 'class MeanEmbeddingVectorizer(object):\\n    \"\"\"\\n ...)], axis=0)\\n            for words in X\\n        ])', 'class TfidfEmbeddingVectorizer(object):\\n    def ...=0)\\n                for words in X\\n            ])', 'from sklearn.pipeline import Pipeline\\nfrom sklea...xtraTreesClassifier(n_estimators=200,n_jobs=7))])', \"# Create pipeline\\nfrom sklearn.feature_extractio...  ('clf', MultinomialNB())\\n                    ])\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", \"from sklearn.model_selection import cross_val_sc...lf, X_train, y_train, cv=5,n_jobs=7,scoring='f1')\", \"X = df['tokenized_sents'].tolist()\", \"from sklearn.model_selection import train_test_s...is_clinician'] , test_size=0.2, random_state=329)\", 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s..., X_train, y_train, cv=5,n_jobs=7,scoring=\\'f1\\')\")', 'get_ipython().run_cell_magic(\\'time\\', \\'\\', \"from s...ain, y_train, cv=5,n_jobs=7,scoring=\\'f1_macro\\')\")', 'print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))', ...], 'MeanEmbeddingVectorizer': <class '__main__.MeanEmbeddingVectorizer'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {2:                                                 ...  default            NaN  \n\n[2 rows x 21 columns], 25: [['for', 'a', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', '7', 'hours', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', '', 'i', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'a', 'lot', 'of', 'things.', '', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', 'a', '6,', 'the', 'pain', 'during', 'sex', 'was', 'a', '9', '(we', 'had', 'to', 'stop', ...]], 29: 0, 31: [['for', 'manlet', 'such', 'as', 'yourself', \"i'd\", 'recommend', 'at', 'least', '70', 'oz', 'of', 'water', 'daily', 'and', 'at', 'least', 'hours', 'of', 'sleep.', ...], ['thank', 'you', 'very', 'much', 'for', 'answering!'], ['never', 'been', 'tested', 'for', 'that.', 'was', 'hoping', 'the', 'full', 'blood', 'test', 'would', 'reveal', 'lot', 'of', 'things.', 'am', 'gonna', 'find', 'good', ...], ['she', 'said', 'her', 'constant', 'abdominal', 'pain', 'is', '6,', 'the', 'pain', 'during', 'sex', 'was', '(we', 'had', 'to', 'stop', 'immediately).', 'no', 'fever.'], [\"she's\", 'had', 'it', 'for', 'months,', \"we've\", 'never', 'had', 'any', 'issues', 'with', 'it', 'before.'], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], [\"here's\", 'the', 'full', 'enchilada:dimensions/calcs', 'bsa', 'adjustedlvidd', 'm-mode', '5.7', 'cm', '(3.5-5.7)', 'lvidd', 'm-mode', '2.89', '(1.9-3.2)', 'lvidd', '2d', '4.83', 'cm', '(3.5-5.7)', 'lvidd', ...], ['the', 'attending', 'at', 'the', 'hospital', 'said', 'they', 'wanted', 'to', 'do', 'follow-up', 'ekg.', 'was', 'hesitant', 'because', 'my', 'insurance', 'sucks', 'and', \"it's\", ...], ['appreciate', 'the', 'clarification.', 'thanks', 'for', 'all', 'your', 'help!'], ['laying', 'here', 'due', 'to', 'severe', 'sciatica', 'and', 'meralgia', 'paresthetica.', 'seriously', 'sympathize', 'and', 'empathize', 'your', 'pain.', 'bless', 'you.']]}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'QuickUMLS': <class 'quickumls.QuickUMLS'>, 'TfidfEmbeddingVectorizer': <class '__main__.TfidfEmbeddingVectorizer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n        local_ns = None\n   1239             except:\n   1240                 self.shell.showtraceback()\n   1241                 return\n   1242             end = clock2()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/Projects/Automated-Health-Responses/notebooks/<timed exec> in <module>()\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, groups=None, scoring='f1_macro', cv=5, n_jobs=7, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, groups=None, scoring={'score': make_scorer(f1_score, pos_label=None, average=macro)}, cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=7, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=5, random_state=None, shuffle=False)>\n        X = [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...]\n        y = 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=7), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=7)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nTypeError                                          Sat May 26 15:49:09 2018\nPID: 48691Python 3.6.5: /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), [['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], 173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, {'score': make_scorer(f1_score, pos_label=None, average=macro)}, memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), array([    0,     1,     2, ..., 89313, 89314, 89321]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['&gt;', 'did', 'it', 'start', 'with', 'raised', 'roof', 'vesicle?', 'no'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['no', 'problem-', 'was', '18', 'when', 'it', 'happened', 'to', 'me,', 'so', 'recognize', 'the', 'symptoms.', 'glad', 'to', 'hear', 'it', 'got', 'caught', 'before', ...], ['muscle', 'spasms.', 'see', 'massage', 'therapist.', 'the', 'muscles', 'you', 'need', 'to', 'stretch', 'are', 'at', 'the', 'front.'], ['yep.', 'stop', 'playing', 'with', 'fire.'], ['still', 'think', 'having', 'your', 'care', 'coordinated', 'by', 'pain', 'management', 'dr.', 'is', 'the', 'way', 'to', 'go.', 'they', 'know', 'their', 'stuff.', \"i'm\", ...], ['am', 'underweight', 'and', 'pretty', 'much', 'ways', 'have', 'been,', 'foot', 'and', 'just', 'under', 'stone.my', 'eating', 'and', 'weight', 'has', 'always', 'had', 'fluctuations,', ...], ['you', 'know', 'that', 'very', 'well', 'is', 'my', 'issue,i', 'was', 'having', 'mystery', 'symptoms', 'before', 'and', 'they', 'stopped', 'when', 'was', 'prescribed', 'lexapro', ...], ['lol,', 'thought', 'you', 'took', 'bunch', 'of', 'them.', \"you're\", 'going', 'to', 'be', 'fine.', 'you', \"didn't\", 'take', 'too', 'much', 'of', 'anything.', \"you're\", ...], ['that', 'would', 'take', 'all', 'of', 'my', 'willpower', 'to', 'not', 'pull', 'that', 'thing', 'right', 'off...'], ['ok', 'thanks!'], ['who', 'doni', 'trust', 'in', 'this', 'sub.', 'unverified', 'vs', 'verified?'], ['it', 'looks', 'like', 'it', 'has', 'exfoliating', 'properties', 'through', 'urea.', 'probably', 'better', 'to', 'use', 'something', 'like', 'aquaphor', 'on', 'the', 'penis.'], ['**your', 'post', 'has', 'been', 'removed', 'because', 'you', 'did', 'not', 'abide', 'to', 'our', 'detailed', 'submissions', 'requirement', 'for', 'posting.', 'please', 'include', 'as', ...], ['[deleted]'], ['[deleted]'], ['just', 'looks', 'like', 'normal', 'skin', ':)if', 'some', 'get', 'inflamed', 'may', 'be', 'keratosis', 'pilaris', 'try', 'high-urea', 'moisturisers', 'but', \"don't\", 'worry', ':)'], ['the', 'only', 'thing', 'about', 'reducing', 'smoking', 'that', 'could', 'possibly', 'cause', 'an', 'increase', 'in', 'illness', 'is', 'stress.', 'but', \"don't\", 'think', 'it', ...], [\"don't\", 'feel', 'any', 'open', 'wounds', 'but', 'there', 'is', 'irritation', 'the', 'blood', 'is', 'light', 'red/', 'dark', 'pink', 'colour.'], ['[deleted]'], ...], y=173155    0\n302805    1\n281390    0\n261886    0\n...\nName: is_clinician, Length: 446118, dtype: int64, scorer={'score': make_scorer(f1_score, pos_label=None, average=macro)}, train=memmap([ 89172,  89173,  89174, ..., 446115, 446116, 446117]), test=array([    0,     1,     2, ..., 89313, 89314, 89321]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X_train = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y_train = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...(alpha=1.0, class_prior=None, fit_prior=True))])>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...B(alpha=1.0, class_prior=None, fit_prior=True))]), X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x1d8f811e0>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = [['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...]\n        y = 478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], y=478446    0\n112472    0\n428794    0\n105361    0\n...\nName: is_clinician, Length: 356894, dtype: int64)\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=[['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...], ['have', 'never', 'heard', 'of', 'snyarel.', 'is', 'this', 'something', 'could', 'use', 'instead', 'of', 'lupron?', 'sorry', 'for', 'all', 'the', 'questions.', 'appreciate', 'your', ...], [\"i'm\", 'not', 'saying', \"it's\", 'not', 'that,', 'but', \"i've\", 'had', 'simmilar', 'problems', 'since', 'way', 'before', 'starting', 'with', 'finasteride.', 'the', 'thing', 'is,', ...], ['they', \"didn't,\", 'no.', 'the', 'urologist', 'did', 'about', 'six', 'different', 'urine', 'tests', 'and', 'all', 'came', 'back', 'normal.', 'he', 'also', 'did', 'bladder', ...], ['you', 'honestly', 'have', 'to', 'keep', 'trying', 'different', 'ones', 'until', 'one', 'works', 'for', 'you.', 'took', 'me', 'years.'], ['try', 'using', 'betadine', 'in', 'between', 'the', 'toes', 'it', 'should', 'dry', 'it', 'out'], ['thank', 'you', 'for', 'easing', 'my', 'mind'], [\"i've\", 'been', 'eating', 'like', 'shit', 'for', 'the', 'past', '2-3', 'weeks.', 'need', 'to', 'get', 'my', 'ass', 'to', 'the', 'gym', 'as', 'well.', ...], ['have', 'not', 'but', \"i'll\", 'give', 'it', 'go.', 'have', 'cream', 'in', 'my', 'coffee', 'pretty', 'regularly', 'with', 'no', 'issue,', 'but', 'guess', 'that', ...], ['please', 'go', 'to', 'the', 'er', 'immediately.', 'this', 'sounds', 'very', 'much', 'like', 'what', 'the', 'symptoms', 'are;', 'in', 'fact,', 'what', 'you', 'describe', ...], ['hello!', 'hope', 'you', 'are', 'doing', 'well.considering', 'that', 'the', 'wound', 'is', 'not', 'healing', 'like', 'the', 'others', 'it', 'would', 'be', 'wise', 'to', ...], ['the', 'mri', 'was', 'performed', 'at', 'the', 'end', 'of', 'august,', 'and', \"haven't\", 'heard', 'anything', 'from', 'my', 'gp,', 'so', \"i'm\", 'assuming', \"it's\", ...], ['thanks,', \"that's\", 'what', 'figured,', 'but', 'some', 'people', 'are', 'just', 'too', 'stubborn.', \"i'll\", 'try', 'to', 'get', 'her', 'there', 'asap.'], ['sorry', 'to', 'hear', 'about', 'your', 'injury.', 'last', 'august,', 'slipped', 'and', 'dislocated', 'my', 'knee,', 'which', 'was', 'the', 'most', 'painful', 'thing', 'in', ...], [\"don't\", '\"consider\"', 'avoiding', 'it.', 'avoid', 'it.', 'about', '30%', 'of', 'the', 'population', 'that', 'experiments', 'with', 'cocaine', 'becomes', 'life', 'log', 'addict.', 'there', ...], ['haha', 'oh', \"i've\", 'been', 'teasing', 'him', 'since', 'we', 'were', 'in', 'grade', '6;', \"he's\", 'so', 'damn', 'stupid,', \"he'll\", 'do', 'anything', 'you', ...], ['might', 'be', 'magnesium', 'deficiency.', 'very', 'common', 'and', 'easy', 'to', 'just', 'supplement.', 'for', 'any', 'musculoskeletal', 'problem,', 'mg', 'seems', 'to', 'do', 'wonders.', ...], ['gender,', 'age,', 'weight,', 'etc?', 'did', 'they', 'do', 'any', 'imaging?'], ['luxiq', 'is', 'betamethasone.', \"it's\", 'steroid.ciclopirox', 'and', 'ketoconazole', 'are', 'antifungals.together,', 'they', 'reduce', 'inflammation', '(steroids)', 'and', 'kill', 'fungus', '(such', 'as', 'candida', 'or', ...], ['also,', 'should', 'request', 'second', 'opinion', 'on', 'this', 'diagnosis.'], ...], fixed_vocab=False)\n    787         indptr = _make_int_array()\n    788         values = _make_int_array()\n    789         indptr.append(0)\n    790         for doc in raw_documents:\n    791             feature_counter = {}\n--> 792             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    793                 try:\n    794                     feature_idx = vocabulary[feature]\n    795                     if feature_idx not in feature_counter:\n    796                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    261         elif self.analyzer == 'word':\n    262             stop_words = self.get_stop_words()\n    263             tokenize = self.build_tokenizer()\n    264 \n    265             return lambda doc: self._word_ngrams(\n--> 266                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    267 \n    268         else:\n    269             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    270                              self.analyzer)\n\n...........................................................................\n/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc=['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...])\n    236     def build_tokenizer(self):\n    237         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n    238         if self.tokenizer is not None:\n    239             return self.tokenizer\n    240         token_pattern = re.compile(self.token_pattern)\n--> 241         return lambda doc: token_pattern.findall(doc)\n        doc = ['couple', 'of', 'weeks,', 'but', 'even', 'before', 'that', 'had', 'dry', 'heaving', 'mostly', 'in', 'the', 'mornings,', 'bu', 'didnt', 'think', 'much', 'of', 'it', ...]\n    242 \n    243     def get_stop_words(self):\n    244         \"\"\"Build or fetch the effective stop words list\"\"\"\n    245         return _check_stop_list(self.stop_words)\n\nTypeError: expected string or bytes-like object\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(text_clf, X_train, y_train, cv=5,n_jobs=7,scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-05d64caf78a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 macro: %0.2f (+/- %0.2f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"F1 macro: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
