{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope\n",
    "\n",
    "This notebook goes through how to create a training set from a set of questions and answers\n",
    "\n",
    "## Optional: further research\n",
    "* **OOB for cleaning text SpaCy** https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49\n",
    "* 1) BLUE SCORE FOR EVALUATION: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "* 2) Stanford SQuad (Question/Answer Scoring Datasets): https://github.com/obryanlouis/qa\n",
    "* 3) https://github.com/facebookresearch/ParlAI\n",
    "* 4) Seq2Seq: https://docs.chainer.org/en/stable/examples/seq2seq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# Utility\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# NLP\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Custom libraries\n",
    "# from seq2seq_model import Seq2SeqModel\n",
    "# from corpora_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_corpora(path_to_pickle):\n",
    "    data = pickle.load(open(path_to_pickle,'rb'))\n",
    "    qa = []\n",
    "    for i in list(data):\n",
    "        qa.append((i[0]['utterance'],i[1]['utterance']))\n",
    "\n",
    "    # Split pairs into question and answer\n",
    "    question, answer = zip(*qa)\n",
    "    return question,answer\n",
    "\n",
    "def tokenize(txt):\n",
    "    return str(txt).lower().split(' ')\n",
    "\n",
    "def clean_sentences(txt):\n",
    "    # tokenize\n",
    "    tokenized_txt = tokenize(txt)\n",
    "    # Filter comments too long\n",
    "    return tokenized_txt[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: All responses are equal to the original query except for the comment by the author\n",
    "# Zipped Q/A\n",
    "question,answer = retrieve_corpora('../data/all_responses_equal.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Husband deteriorating before my eyes, doctors at a loss, no one will help; Reddit docs, I need you.\n",
      "A: I really do wish the best for your husband and hope you eventually get through this! keep your and your husband's head up and tell him he has people cheering for him sure, including me! :)\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(\"Q:\", question[idx])\n",
    "print(\"A:\", answer[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108825\n",
      "CPU times: user 996 ms, sys: 76.1 ms, total: 1.07 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean sentences\n",
    "clean_question = [clean_sentences(s) for s in question]\n",
    "clean_answer = [clean_sentences(s) for s in answer]\n",
    "\n",
    "print(len(clean_question))\n",
    "assert len(clean_question) == len(clean_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ['husband', 'deteriorating', 'before', 'my', 'eyes,', 'doctors', 'at', 'a', 'loss,', 'no', 'one', 'will', 'help;', 'reddit', 'docs,', 'i', 'need', 'you.']\n",
      "A: ['a', 'few', 'weeks', 'ago', 'i', 'was', 'suffering', 'from', 'the', 'same', 'symptoms.', 'severe', 'groin', 'pain', 'that', 'radiated', 'through', 'my', 'testicles', 'and']\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(\"Q:\", clean_question[idx])\n",
    "print(\"A:\", clean_answer[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if min_len <= len(sentences_l1[i]) <= max_len and \\\n",
    "                 min_len <= len(sentences_l2[i]) <= max_len:\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "    return filtered_sentences_l1, filtered_sentences_l2\n",
    "\n",
    "import data_utils\n",
    "\n",
    "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "    count_words = Counter()\n",
    "    dict_words = {}\n",
    "    opt_dict_size = len(data_utils.OP_DICT_IDS)\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            count_words[word] += 1\n",
    "    \n",
    "    dict_words[data_utils._PAD] = data_utils.PAD_ID\n",
    "    dict_words[data_utils._GO] = data_utils.GO_ID\n",
    "    dict_words[data_utils._EOS] = data_utils.EOS_ID\n",
    "    dict_words[data_utils._UNK] = data_utils.UNK_ID\n",
    "\n",
    "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "        dict_words[item[0]] = idx + opt_dict_size\n",
    "    if storage_path:\n",
    "        pickle.dump(dict_words, open(storage_path, \"wb\"))\n",
    "    return dict_words\n",
    "\n",
    "def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "    indexed_sentences = []\n",
    "    not_found_counter = 0\n",
    "    for sent in sentences:\n",
    "        idx_sent = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                idx_sent.append(indexed_dictionary[word])\n",
    "            except KeyError:\n",
    "                idx_sent.append(data_utils.UNK_ID)\n",
    "                not_found_counter += 1\n",
    "        indexed_sentences.append(idx_sent)\n",
    "    \n",
    "    print('[sentences_to_indexes] Did not find {} words'.format(not_found_counter))\n",
    "    return indexed_sentences\n",
    "\n",
    "def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
    "    assert len(sentences_l1) == len(sentences_l2)\n",
    "    data_set = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        padding_l1 = len_l1 - len(sentences_l1[i])\n",
    "        pad_sentence_l1 = ([data_utils.PAD_ID]*padding_l1) + sentences_l1[i]\n",
    "        padding_l2 = len_l2 - len(sentences_l2[i])\n",
    "        pad_sentence_l2 = [data_utils.GO_ID] + sentences_l2[i] + [data_utils.EOS_ID] + ([data_utils.PAD_ID] * padding_l2)\n",
    "        data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
    "    return data_set\n",
    "\n",
    "def extract_max_length(corpora):\n",
    "    return max([len(sentence) for sentence in corpora])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Filtered Corpora length (i.e. number of sentences)\n",
      "108825\n"
     ]
    }
   ],
   "source": [
    "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_question, \n",
    "          clean_answer)\n",
    "print(\"# Filtered Corpora length (i.e. number of sentences)\")\n",
    "print(len(filt_clean_sen_l1))\n",
    "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sentences_to_indexes] Did not find 28670 words\n",
      "[sentences_to_indexes] Did not find 98910 words\n",
      "# Same sentences as before, with their dictionary ID\n",
      "Q: [('husband', 456), ('deteriorating', 1371), ('before', 191), ('my', 5), ('eyes,', 923), ('doctors', 69), ('at', 49), ('a', 6), ('loss,', 617), ('no', 47), ('one', 94), ('will', 81), ('help;', 1333), ('reddit', 575), ('docs,', 829), ('i', 4), ('need', 36), ('you.', 1322)]\n",
      "A: [('hey,', 1011), (\"how's\", 2487), ('your', 14), ('husband', 1356), ('doing', 256), ('now?', 1425), ('hope', 274), ('everything', 416), ('is', 11), ('okay.', 957)]\n"
     ]
    }
   ],
   "source": [
    "dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\")\n",
    "dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=15000, storage_path=\"/tmp/l2_dict.p\")\n",
    "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "print(\"# Same sentences as before, with their dictionary ID\")\n",
    "print(\"Q:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n",
    "print(\"A:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Max sentence sizes:\n",
      "DE: 20\n",
      "EN: 20\n"
     ]
    }
   ],
   "source": [
    "max_length_l1 = extract_max_length(idx_sentences_l1)\n",
    "max_length_l2 = extract_max_length(idx_sentences_l2)\n",
    "print(\"# Max sentence sizes:\")\n",
    "print(\"DE:\", max_length_l1)\n",
    "print(\"EN:\", max_length_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step, let's add paddings and markings to the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prepared minibatch with paddings and extra stuff\n",
      "Q: [0, 0, 456, 1371, 191, 5, 923, 69, 49, 6, 617, 47, 94, 81, 1333, 575, 829, 4, 36, 1322]\n",
      "A: [1, 1011, 2487, 14, 1356, 256, 1425, 274, 416, 11, 957, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "# The sentence pass from X to Y tokens\n",
      "Q: 18 -> 20\n",
      "A: 10 -> 22\n"
     ]
    }
   ],
   "source": [
    "data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n",
    "print(\"# Prepared minibatch with paddings and extra stuff\")\n",
    "print(\"Q:\", data_set[0][0])\n",
    "print(\"A:\", data_set[0][1])\n",
    "print(\"# The sentence pass from X to Y tokens\")\n",
    "print(\"Q:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\n",
    "print(\"A:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we're done with the corpora, it's now time to work on the model. This project requires again a sequence to sequence model, therefore we can use an RNN. Even more, we can reuse part of the code from the previous project: we'd just need to change how the dataset is built, and the parameters of the model. We can then copy the training script built in the previous chapter, and modify the build_dataset function, to use the Cornell dataset.\n",
    "\n",
    "Mind that the dataset used in this chapter is bigger than the one used in the previous, therefore you may need to limit the corpora to a few dozen thousand lines. On a 4 years old laptop with 8GB RAM, we had to select only the first 30 thousand lines, otherwise, the program ran out of memory and kept swapping. As a side effect of having fewer examples, even the dictionaries are smaller, resulting in less than 10 thousands words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import data_utils\n",
    "\n",
    "\n",
    "def tokenize(txt):\n",
    "    return str(txt).lower().split(' ')\n",
    "\n",
    "def clean_sentence(txt):\n",
    "    # tokenize\n",
    "    tokenized_txt = tokenize(txt)\n",
    "    # Filter comments too long\n",
    "    return tokenized_txt[:20]\n",
    "\n",
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    import data_utils\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if min_len <= len(sentences_l1[i]) <= max_len and \\\n",
    "                 min_len <= len(sentences_l2[i]) <= max_len:\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "    return filtered_sentences_l1, filtered_sentences_l2\n",
    "\n",
    "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "    count_words = Counter()\n",
    "    dict_words = {}\n",
    "    opt_dict_size = len(data_utils.OP_DICT_IDS)\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            count_words[word] += 1\n",
    "    \n",
    "    dict_words[data_utils._PAD] = data_utils.PAD_ID\n",
    "    dict_words[data_utils._GO] = data_utils.GO_ID\n",
    "    dict_words[data_utils._EOS] = data_utils.EOS_ID\n",
    "    dict_words[data_utils._UNK] = data_utils.UNK_ID\n",
    "\n",
    "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "        dict_words[item[0]] = idx + opt_dict_size\n",
    "    if storage_path:\n",
    "        pickle.dump(dict_words, open(storage_path, \"wb\"))\n",
    "    return dict_words\n",
    "\n",
    "def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "    indexed_sentences = []\n",
    "    not_found_counter = 0\n",
    "    for sent in sentences:\n",
    "        idx_sent = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                idx_sent.append(indexed_dictionary[word])\n",
    "            except KeyError:\n",
    "                idx_sent.append(data_utils.UNK_ID)\n",
    "                not_found_counter += 1\n",
    "        indexed_sentences.append(idx_sent)\n",
    "    \n",
    "    print('[sentences_to_indexes] Did not find {} words'.format(not_found_counter))\n",
    "    return indexed_sentences\n",
    "\n",
    "def extract_max_length(corpora):\n",
    "    return max([len(sentence) for sentence in corpora])\n",
    "        \n",
    "def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
    "    assert len(sentences_l1) == len(sentences_l2)\n",
    "    data_set = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        padding_l1 = len_l1 - len(sentences_l1[i])\n",
    "        pad_sentence_l1 = ([data_utils.PAD_ID]*padding_l1) + sentences_l1[i]\n",
    "        padding_l2 = len_l2 - len(sentences_l2[i])\n",
    "        pad_sentence_l2 = [data_utils.GO_ID] + sentences_l2[i] + [data_utils.EOS_ID] + ([data_utils.PAD_ID] * padding_l2)\n",
    "        data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
    "    return data_set    \n",
    "\n",
    "def build_dataset(use_stored_dictionary=False,path_to_data=''):\n",
    "    sen_l1, sen_l2 = retrieve_corpora(path_to_data)\n",
    "    clean_sen_l1 = [clean_sentence(s) for s in sen_l1][:30000] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n",
    "    clean_sen_l2 = [clean_sentence(s) for s in sen_l2][:30000] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n",
    "    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2, max_len=10)\n",
    "    \n",
    "\n",
    "    if not use_stored_dictionary:\n",
    "        dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=10000, storage_path=path_l1_dict)\n",
    "        dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=path_l2_dict)\n",
    "    else:\n",
    "        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
    "        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
    "    dict_l1_length = len(dict_l1)\n",
    "    dict_l2_length = len(dict_l2)\n",
    "\n",
    "    idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "    idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "\n",
    "    max_length_l1 = extract_max_length(idx_sentences_l1)\n",
    "    max_length_l2 = extract_max_length(idx_sentences_l2)\n",
    "    data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n",
    "    return (filt_clean_sen_l1, filt_clean_sen_l2), \\\n",
    "           data_set, \\\n",
    "           (max_length_l1, max_length_l2), \\\n",
    "           (dict_l1_length, dict_l2_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few iterations, you can stop the program and you'll see something similar to this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd789d49336a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_stored_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/all_responses_equal.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'build_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "build_dataset(use_stored_dictionary=True,path_to_data='../data/all_responses_equal.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
