{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope\n",
    "\n",
    "This notebook goes through how to create a training set from a set of questions and answers\n",
    "\n",
    "## Optional: further research\n",
    "* 1) BLUE SCORE FOR EVALUATION: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "* 2) Stanford SQuad (Question/Answer Scoring Datasets): https://github.com/obryanlouis/qa\n",
    "* 3) https://github.com/facebookresearch/ParlAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# Utility\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# NLP\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Custom libraries\n",
    "# from seq2seq_model import Seq2SeqModel\n",
    "# from corpora_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unzip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-ef17719e4804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unzip' is not defined"
     ]
    }
   ],
   "source": [
    "zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: All responses are equal to the original query except for the comment by the author\n",
    "# Zipped Q/A\n",
    "data = pickle.load(open('../data/all_responses_equal.p','rb'))\n",
    "q_and_a = []\n",
    "for i in list(data):\n",
    "    q_and_a.append((i[0]['utterance'],i[1]['utterance']))\n",
    "\n",
    "# Split pairs into question and answer\n",
    "question, answer = zip(*q_and_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Husband deteriorating before my eyes, doctors at a loss, no one will help; Reddit docs, I need you.\n",
      "A: A few weeks ago I was suffering from the same symptoms. Severe groin pain that radiated through my testicles and legs, urgent urination even though I didn't need to pee, muscle twitching/eye twitching. Everything came back normal. When I went to the hospital the sixth time, the doctors found that I had a lot of poop stuck in my colon around the pelvis area so they gave me an enema and some miralax and I was instantly cured. all this pain was also giving me anxiety and the anxiety made me feel like my face was going numb and that my legs and arms were going numb. After a few days I was still having trouble sleeping but I'm getting back to normal now. Poop is no joke and I hope that this is the problem your husband is having! If not.... I'm honestly so sorry because having to go through this pain for sooooooo long is insanely depressing. Maybe check out this subreddit /r/chronicpain\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(\"Q:\", question[idx])\n",
    "print(\"A:\", answer[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    return str(txt).split(' ')\n",
    "\n",
    "def clean_sentences(txt):\n",
    "    # tokenize\n",
    "    tokenized_txt = tokenize(txt)\n",
    "    # Filter comments too long\n",
    "    return tokenized_txt[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108825\n",
      "CPU times: user 1.03 s, sys: 70.2 ms, total: 1.1 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_question = [clean_sentences(s) for s in question]\n",
    "clean_answer = [clean_sentences(s) for s in answer]\n",
    "\n",
    "print(len(clean_question))\n",
    "assert len(clean_question) == len(clean_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Filtered Corpora length (i.e. number of sentences)\n",
      "108825\n"
     ]
    }
   ],
   "source": [
    "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_question, \n",
    "          clean_answer)\n",
    "print(\"# Filtered Corpora length (i.e. number of sentences)\")\n",
    "print(len(filt_clean_sen_l1))\n",
    "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's create the dictionaries for the two sets of sentences. Practically, they should look the same (since the same sentence appears once on the left side, and once in the right side) except there might be some changes introduced by the first and last sentences of a conversation (they appear only once). To make the best out of our corpora, let's build two dictionaries of words and then encode all the words in the corpora with their dictionary indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "    count_words = Counter()\n",
    "    dict_words = {}\n",
    "\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            count_words[word] += 1\n",
    "\n",
    "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "        dict_words[item[0]] = idx + dict_size\n",
    "\n",
    "    return dict_words\n",
    "\n",
    "def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "    indexed_sentences = []\n",
    "    not_found_counter = 0\n",
    "    for sent in sentences:\n",
    "        idx_sent = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                idx_sent.append(indexed_dictionary[word])\n",
    "            except KeyError:\n",
    "                idx_sent.append(data_utils.UNK_ID)\n",
    "                not_found_counter += 1\n",
    "        indexed_sentences.append(idx_sent)\n",
    "    \n",
    "    print('[sentences_to_indexes] Did not find {} words'.format(not_found_counter))\n",
    "    return indexed_sentences\n",
    "\n",
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if min_len <= len(sentences_l1[i]) <= max_len and \\\n",
    "                 min_len <= len(sentences_l2[i]) <= max_len:\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "    return filtered_sentences_l1, filtered_sentences_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-9b4c3f6db334>\u001b[0m in \u001b[0;36msentences_to_indexes\u001b[0;34m(sentences, indexed_dictionary)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0midx_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sphincterotomy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8f4a70444361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdict_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_indexed_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdict_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_indexed_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0midx_sentences_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilt_clean_sen_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_l1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0midx_sentences_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilt_clean_sen_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_l2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# Same sentences as before, with their dictionary ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-9b4c3f6db334>\u001b[0m in \u001b[0;36msentences_to_indexes\u001b[0;34m(sentences, indexed_dictionary)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0midx_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0midx_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNK_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mnot_found_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mindexed_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_utils' is not defined"
     ]
    }
   ],
   "source": [
    "dict_l1 = create_indexed_dictionary(clean_question, dict_size=15000)\n",
    "dict_l2 = create_indexed_dictionary(clean_answer, dict_size=15000)\n",
    "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "print(\"# Same sentences as before, with their dictionary ID\")\n",
    "print(\"Q:\", list(zip(clean_question[0], idx_sentences_l1[0])))\n",
    "print(\"A:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
