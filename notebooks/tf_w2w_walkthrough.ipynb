{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "corpus_raw = \"Never been tested for that.  I was hoping the full blood test would reveal a lot of things.  I am gonna find a good doctor and I will ask about it.  And yeah other than the myocarditis.  My heart has no blockages from the tests they did to me.\\n\\nI'm hoping my heart is healing because this esophagus thing is throwing me off.  My cardiologist is clearing me to return to work in 3 weeks, so I'm hoping I'm healing.\\n\\nThank you for your reply :)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in corpus_raw.split(' '):\n",
    "    words.append(word)\n",
    "\n",
    "\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A look at the word pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['he', 'is'],\n",
       " ['he', 'the'],\n",
       " ['he', 'king'],\n",
       " ['is', 'he'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['the', 'he'],\n",
       " ['the', 'is'],\n",
       " ['the', 'king'],\n",
       " ['king', 'he']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 5\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "\n",
    "print('A look at the word pairs')     \n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "(44, 9) (44, 9)\n"
     ]
    }
   ],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  4.0349994\n",
      "loss is :  1.7571597\n",
      "loss is :  1.7024547\n",
      "loss is :  1.6730042\n",
      "loss is :  1.6542072\n",
      "loss is :  1.6398301\n",
      "loss is :  1.6268085\n",
      "loss is :  1.6138527\n",
      "loss is :  1.6006513\n",
      "loss is :  1.5876542\n",
      "loss is :  1.5758338\n",
      "loss is :  1.5660801\n",
      "loss is :  1.5586548\n",
      "loss is :  1.5532161\n",
      "loss is :  1.5492139\n",
      "loss is :  1.5461819\n",
      "loss is :  1.5438062\n",
      "loss is :  1.5418919\n",
      "loss is :  1.5403153\n",
      "loss is :  1.5389947\n",
      "loss is :  1.5378734\n",
      "loss is :  1.5369102\n",
      "loss is :  1.5360749\n",
      "loss is :  1.5353442\n",
      "loss is :  1.5347004\n",
      "loss is :  1.534129\n",
      "loss is :  1.5336193\n",
      "loss is :  1.5331619\n",
      "loss is :  1.5327495\n",
      "loss is :  1.5323757\n",
      "loss is :  1.532036\n",
      "loss is :  1.5317258\n",
      "loss is :  1.5314416\n",
      "loss is :  1.5311805\n",
      "loss is :  1.5309398\n",
      "loss is :  1.5307171\n",
      "loss is :  1.530511\n",
      "loss is :  1.5303193\n",
      "loss is :  1.530141\n",
      "loss is :  1.5299745\n",
      "loss is :  1.5298188\n",
      "loss is :  1.5296727\n",
      "loss is :  1.5295361\n",
      "loss is :  1.5294074\n",
      "loss is :  1.5292863\n",
      "loss is :  1.5291718\n",
      "loss is :  1.5290638\n",
      "loss is :  1.5289617\n",
      "loss is :  1.528865\n",
      "loss is :  1.5287732\n",
      "loss is :  1.5286862\n",
      "loss is :  1.5286034\n",
      "loss is :  1.5285245\n",
      "loss is :  1.5284497\n",
      "loss is :  1.528378\n",
      "loss is :  1.5283097\n",
      "loss is :  1.5282445\n",
      "loss is :  1.5281819\n",
      "loss is :  1.5281224\n",
      "loss is :  1.5280651\n",
      "loss is :  1.5280102\n",
      "loss is :  1.5279576\n",
      "loss is :  1.5279069\n",
      "loss is :  1.5278586\n",
      "loss is :  1.5278118\n",
      "loss is :  1.5277671\n",
      "loss is :  1.5277237\n",
      "loss is :  1.5276818\n",
      "loss is :  1.5276418\n",
      "loss is :  1.5276031\n",
      "loss is :  1.5275657\n",
      "loss is :  1.5275296\n",
      "loss is :  1.5274945\n",
      "loss is :  1.5274609\n",
      "loss is :  1.5274284\n",
      "loss is :  1.5273968\n",
      "loss is :  1.5273663\n",
      "loss is :  1.5273366\n",
      "loss is :  1.5273079\n",
      "loss is :  1.5272801\n",
      "loss is :  1.5272533\n",
      "loss is :  1.527227\n",
      "loss is :  1.5272018\n",
      "loss is :  1.527177\n",
      "loss is :  1.5271531\n",
      "loss is :  1.5271298\n",
      "loss is :  1.5271072\n",
      "loss is :  1.5270852\n",
      "loss is :  1.5270638\n",
      "loss is :  1.5270431\n",
      "loss is :  1.5270228\n",
      "loss is :  1.527003\n",
      "loss is :  1.5269837\n",
      "loss is :  1.526965\n",
      "loss is :  1.5269469\n",
      "loss is :  1.526929\n",
      "loss is :  1.5269116\n",
      "loss is :  1.5268947\n",
      "loss is :  1.5268782\n",
      "loss is :  1.526862\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) #make sure you do this!\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "n_iters = 10000\n",
    "# train for n_iter iterations\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    if _ % 100 == 0:\n",
    "        print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('loss is : ', 1.5268464)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.736077    2.8062186   3.183596    0.01743919  3.1223805 ]\n"
     ]
    }
   ],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors[ word2int['queen'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['queen'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5295392  -0.5451896  -0.16969097 -1.0160415   1.5892736 ]\n",
      " [-0.49725425  0.99824333  1.1152103  -2.3213208   1.0008204 ]\n",
      " [-1.5837543   2.2088406   1.2543161   1.2206218  -0.5804225 ]\n",
      " [-0.72303563  1.0704036  -0.32942784  0.32001567  2.528164  ]\n",
      " [-2.736077    2.8062186   3.183596    0.01743919  3.1223805 ]\n",
      " [-3.6587863  -1.0808156   3.167647   -1.5710714   2.0218534 ]\n",
      " [-1.8690959  -2.0187182   0.7455045   0.51763034  0.28952694]\n",
      " [-2.3066306  -0.6539742   2.2283645  -0.36019272  2.9689565 ]\n",
      " [-0.39887297  1.088914    4.094547    0.8547706   2.9263477 ]]\n"
     ]
    }
   ],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.76670915\n",
      "the -0.758998\n",
      "king -0.9981212\n",
      "is -0.6416782\n",
      "queen 0.9291776\n",
      ". -0.5889759\n",
      "he 0.20023805\n",
      "royal -0.0108503215\n",
      "she -0.16225831\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
