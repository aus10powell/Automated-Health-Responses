{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense,GRU,Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using from 2 sites:\n",
    "*Main: https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "*Second https://stackoverflow.com/questions/49477097/keras-seq2seq-word-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_clean(corpus):\n",
    "    #Normalize tabs and remove newlines\n",
    "    no_tabs = str(corpus).replace('\\t', ' ').replace('\\n', '')\n",
    "\n",
    "    #Remove all characters except A-Z and a dot.\n",
    "    alphas_only = re.sub(\"[^a-zA-Z\\.\\,\\']\", \" \", no_tabs)\n",
    "\n",
    "    #Normalize spaces to 1\n",
    "    multi_spaces = re.sub(\" +\", \" \", alphas_only)\n",
    "    \n",
    "    # reduce space between end of sentence\n",
    "    periods = re.sub(\" \\.\", \".\",multi_spaces)\n",
    "    \n",
    "    return periods.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96112\n"
     ]
    }
   ],
   "source": [
    "query_response = pickle.load(open(\"../data/query_response_direct.p\",\"rb\"))\n",
    "print(len(query_response))\n",
    "query_response = [(minimal_clean(q),minimal_clean(r)) for q,r in query_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"pertinent facts age sex male height ' cm weight lbs, has fluctuated in the last six months from to. race caucasian duration of complaint months details below location geographic dallas, tx on body virtually everywhere details below current medications elmiron mg day hydroxyzine mg day trazodone mg day duloxetine mg day mirtazapine mg day gabapentin mg, day tramadol as needed levetiracetam keppra mg day diazepam mg as needed specialists seen urologist, neurologist, psychiatrist, psychologist, gastroenterologist, internist, er diagnostics run two abdominal ct scans one with contrast, one without , chest x ray, lumbar puncture with emergency blood patch following , endoscopy, colonscopy, cystoscopy with hydrodistention, full spinal mri without contrast , at least six full scale blood draws, urinalysis, other 'minor' testsi will try to keep this as brief as possible.last march, my husband z had what he thought initially was a bladder infection following a kidney stone feeling of urgency, slight burning during urination, cramping pain in bladder area, etc. urinalysis did not show infection, but gp gave course of antibiotics anyway. antibiotics did nothing, so we went to see a real urologist. during the first month or two of different rounds of initial testing treatment options, z's bladder pain got worse, spread to his testicles, started radiating out around his lower abdomen. we also went to an internist who ran a full battery of blood tests and found nothing wrong but a slight vitamin d deficiency. he took some prescription vitd pills for a month, as directed, and when we went back, he was fine, so the internist sent us back to the urologist. uro ordered abdominal ct, which showed nothing, and referred him to a gastroenterologist.gastro ran some tests, found nothing conclusive, did a full endoscopy colonoscopy, the prep for which nearly landed z in the er he was violently leaking from both ends, blood pressure dropped to iirc , lost all color, feverish but he slept until it was time for his procedure. they found nothing but the healthy insides of a year old male. biopsies were done, tissues all clear. gastro sent him back to uro.immediately following the colonoscopy endoscopy, z stopped being able to eat meat of any sort. if he got meat near his mouth, he would nearly vomit. even the smell of meat became too much for him to handle. a week after that, he stopped feeling hungry at all, ever, and began having to simply force himself to eat as a mechanical action rather than because he was hungry or because something tasted good. he had always been a slender guy, but he went from a healthy slim of lbs to lbs in about a month.also about then, he starting experiencing intermittent pain tingling numbness in his upper left thigh much later diagnosed as meralgia paresthetica, despite having none of the risk factors for it.when we went back to uro, he theorized that it was prostatitis and put him on flomax. three months went by, bladder symptoms got worse, and sexual dysfunction started. erections were still just as possible as before, but moderately uncomfortable in testicles and radiating back into the bladder lower abdomen ejaculation caused severe pain both immediately and lasting through the next one to three days. since symptoms were getting worse instead of better, uro theorized it might be interstitial cystitis and performed a cystoscopy with hydrodistention. z's bladder showed no objective signs of ic hunner's ulcers , but did show significantly reduced elasticity and capacity. symptomatic and circumstantial diagnosis of ic was tentatively given and z was placed on elmiron the only drug currently on the market to help treat ic. he was also prescribed tramadol and hydroxyzine to manage the bladder pain.right around this time, z started exhibiting signs of acute depression and anxiety as well as not being able to sleep, so we started seeing a psychiatrist. he went through half a dozen meds i don't remember them all anymore, sorry before finally two months or so ago landing on his course of duloxetine and mirtazapine neither of which has actually helped him sleep or feel less depressed or anxious.uro recommended a neurologist for the leg pain, which grew so bad in november z had to start walking with a cane. neuro tested nerve conduction and found nothing wrong. new battery of blood tests was done, came back fine. lumbar puncture was done, came back fine though he did have to have a blood patch done after the spinal tap that was fun... s neuro prescribed gabapentin to help with the leg pain, and it does help, though the weakness lag as z describes it is still there.for the last six months, he has been experiencing gradually worsening facial muscle twitching and dysarthria, as well as occasional hand tremors and myoclonic jerking.throughout all this, we've had to go to the er three or four times for some sudden drastic swing in his condition. several different blood tests, mris, cts, and even a chest x ray have been done in the hospital. all have come back clear.last month, worried that this might be multiple sclerosis, neuro ordered a spinal mri though without contrast. it did not show evidence of lesions. neuro officially threw his hands up and said, go to the mayo clinic there is nothing more i can do, but there is clearly something wrong. mayo clinic declined to accept an appointment. i have requested an appointment at scott and white, too, but they seem about to decline as well.last week, z started convulsing. at first it was no worse than his usual myoclonus, just more frequent, but by friday it was so violent and constant that he could not sleep for more than an hour if that before his body started spasming. by tuesday it looked like he was being tazed or poked with a cattle prod. i do have a video of this, but i am reticent to post it on the internet. his neuro advised over the phone that i take him to the er. so i did.er doc ran another set of blood tests, all fine, and gave him some keppra and valium to get the seizing under control and allow him to sleep. as soon as the meds kicked in, he was out.at first this was a blessing, since he hasn't had any proper sleep in over a month probably more and hasn't effectively slept at all for nearly a week. but it's now thursday night nearly hours after the er trip and z has been asleep for all but six of those hours. while he was up today, he complained that he was having double vision, not some of the time but all the time, even when he tried to focus past it. and then he went back to bed.reddit docs, i don't know where to go or what to do anymore. i have no one left to turn to, no more resources to seek out, and no options left apparently. the only thing that had given me hope was that the mayo clinic would help, and they simply refused to even look at him. i'm afraid he's dying a slow, decaying death and i don't know why or how to help him.my husband isn't famous, and he isn't hot , and he isn't important or special or anything else that gets you virally boosted these days... but he's my husband. he's special to me, and i'm at a loss.help please \",\n",
       " \"hey, how's your husband doing now hope everything is okay.\")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2000\n",
      "Number of unique input tokens: 15875\n",
      "Number of unique output tokens: 12976\n",
      "Max sequence length for inputs: 11922\n",
      "Max sequence length for outputs: 9575\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 2000 # Number of samples to train on.\n",
    "\n",
    "\n",
    "#Vectorize the data for words\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "\n",
    "lines = query_response\n",
    "# Option 2\n",
    "for seq1, seq2 in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = seq1, seq2\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '<BOS> ' + target_text + ' <EOS>' #'<BOS>' + target_text + '<EOS>'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # Only one input word\n",
    "    input_text_list = input_text.split()\n",
    "    if len(input_text_list) == 1:\n",
    "        input_words.add(input_text_list[0])\n",
    "    # Multiple input words\n",
    "    else:\n",
    "        for word in input_text_list:\n",
    "            input_words.add(word)\n",
    "                \n",
    "    # For right now we are only looking at 1 target...the medication\n",
    "    target_text_list = target_text.split()\n",
    "    for word in target_text_list:\n",
    "        target_words.add(word)\n",
    "\n",
    "# Option 2\n",
    "input_words = sorted(list(input_words))\n",
    "target_words = sorted(list(target_words))\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)\n",
    "# set max input sequence (may need to pad...)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_encoder_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fa25a1f62904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define an input sequence and process it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We discard `encoder_outputs` and only keep the states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_encoder_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 8  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True,dropout=.1,recurrent_dropout=0.1)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input =  # The cardinality of the input sequence, e.g. number of features, words, or characters for each time step.\n",
    "n_output = # The cardinality of the output sequence, e.g. number of features, words, or characters for each time step.\n",
    "n_units = 256 # The number of cells to create in the encoder and decoder models, e.g. 128 or 256.\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(sentenceLength,), name=\"Encoder_input\")\n",
    "encoder = LSTM(n_units, return_state=True, name='Encoder_lstm') \n",
    "Shared_Embedding = Embedding(output_dim=embedding, input_dim=vocab_size, name=\"Embedding\") \n",
    "word_embedding_context = Shared_Embedding(encoder_inputs) \n",
    "encoder_outputs, state_h, state_c = encoder(word_embedding_context) \n",
    "encoder_states = [state_h, state_c] decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True, name=\"Decoder_lstm\") \n",
    "word_embedding_answer = Shared_Embedding(decoder_inputs) decoder_outputs, _, _ = decoder_lstm(word_embedding_answer, initial_state=encoder_states) \n",
    "decoder_dense = Dense(vocab_size, activation='softmax', name=\"Dense_layer\") \n",
    "decoder_outputs = decoder_dense(decoder_outputs) \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "encoder_model = Model(encoder_inputs, encoder_states) \n",
    "decoder_state_input_h = Input(shape=(n_units,), name=\"H_state_input\") \n",
    "decoder_state_input_c = Input(shape=(n_units,), name=\"C_state_input\") \n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] \n",
    "decoder_outputs, state_h, state_c = decoder_lstm(word_embedding_answer, initial_state=decoder_states_inputs) \n",
    "decoder_states = [state_h, state_c] \n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1, 6, 51) (100000, 1, 3, 51) (100000, 1, 3, 51)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (100000, 1, 6, 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-957ff21a87d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;31m# evaluate LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (100000, 1, 6, 51)"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\tfor _ in range(n_samples):\n",
    "\t\t# generate source sequence\n",
    "\t\tsource = generate_sequence(n_in, cardinality)\n",
    "\t\t# define padded target sequence\n",
    "\t\ttarget = source[:n_out]\n",
    "\t\ttarget.reverse()\n",
    "\t\t# create padded input target sequence\n",
    "\t\ttarget_in = [0] + target[:-1]\n",
    "\t\t# encode\n",
    "\t\tsrc_encoded = to_categorical([source], num_classes=cardinality)\n",
    "\t\ttar_encoded = to_categorical([target], num_classes=cardinality)\n",
    "\t\ttar2_encoded = to_categorical([target_in], num_classes=cardinality)\n",
    "\t\t# store\n",
    "\t\tX1.append(src_encoded)\n",
    "\t\tX2.append(tar2_encoded)\n",
    "\t\ty.append(tar_encoded)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "\t# define training encoder\n",
    "\tencoder_inputs = Input(shape=(None, n_input))\n",
    "\tencoder = LSTM(n_units, return_state=True)\n",
    "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\tencoder_states = [state_h, state_c]\n",
    "\t# define training decoder\n",
    "\tdecoder_inputs = Input(shape=(None, n_output))\n",
    "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\tdecoder_dense = Dense(n_output, activation='softmax')\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\t# define inference encoder\n",
    "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
    "\t# define inference decoder\n",
    "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\tdecoder_states = [state_h, state_c]\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\t# return all models\n",
    "\treturn model, encoder_model, decoder_model\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "\t# encode\n",
    "\tstate = infenc.predict(source)\n",
    "\t# start of sequence input\n",
    "\ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(yhat[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_seq = yhat\n",
    "\treturn array(output)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# configure problem\n",
    "n_features = 50 + 1\n",
    "n_steps_in = 6\n",
    "n_steps_out = 3\n",
    "# define model\n",
    "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# generate training dataset\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "print(X1.shape,X2.shape,y.shape)\n",
    "# train model\n",
    "train.fit([X1, X2], y, epochs=1)\n",
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "\t\tcorrect += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# spot check some examples\n",
    "for _ in range(10):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tprint('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[0]), one_hot_decode(y[0]), one_hot_decode(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
