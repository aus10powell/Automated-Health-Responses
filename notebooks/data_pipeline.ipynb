{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Purpose\n",
    "Currently being used to shortcut creating data for training.\n",
    "\n",
    "*However:* A more rigerous method for cleaning data and make a pipeline where changes can produce better results in training.\n",
    "\n",
    "Use as reference: https://www.safaribooksonline.com/library/view/hands-on-automated-machine/9781788629898/ccac8d45-a703-42b9-992c-d82eafafe94d.xhtml\n",
    "\n",
    "* use as example for text utilities: https://github.com/openai/finetune-transformer-lm/blob/master/text_utils.py\n",
    "\n",
    "\n",
    "\n",
    "### Pipeline considerations:\n",
    "* 1) Cleaning\n",
    "    * Stop words\n",
    "    * newline, tab \n",
    "    * Lowercasing\n",
    "* 2) Normalizing\n",
    "    * Splitting stemmed words into stem and removed portion\n",
    "    * Restricting the length of sentences (query and/or response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "punctuations = string.punctuation\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline_sentencize = spacy.load('en')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "class QueryResponseNormalizer:\n",
    "    \"\"\"\n",
    "    Able to initialize parameters for cleansing and normalizing both query and response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pass optional parameters to \n",
    "    def __init__(self,\n",
    "                remove_stop=False,\n",
    "                lowercase=True,\n",
    "                stem = 'stem_split',\n",
    "                num_sent_words = 100):\n",
    "        self.remove_stop = remove_stop\n",
    "        # optionally lowercase all words\n",
    "        self.lowercase = lowercase\n",
    "        # options are: None (don't do anything), stem (stem but remove extra) and stem_split (stem and add extra)\n",
    "        self.stem = stem\n",
    "        # limits the number of words (not tokens) in a query\n",
    "        self.num_sent_words = num_sent_words\n",
    "    \n",
    "    def newline_replace(self,txt_blob):\n",
    "        \"\"\"\n",
    "        Recursive function deals with multiple newline replacement.\n",
    "        \"\"\"\n",
    "        txt_blob = txt_blob.replace('\\n\\n','\\n')\n",
    "        if '\\n\\n' in txt_blob:\n",
    "            self.newline_replace(txt_blob)\n",
    "        else:\n",
    "            txt_blob = txt_blob.replace('\\n',' ')\n",
    "            return txt_blob  \n",
    "        \n",
    "    def newline_join(self,doc):\n",
    "        \"\"\"\n",
    "        Function deals with issue of having a hyphenatic word connected to a word after a newline.\n",
    "        \"\"\"\n",
    "        new_doc = []\n",
    "        for token in doc.split(' '):\n",
    "            # strip whitespace\n",
    "            token = token.strip()\n",
    "            # join words if necessary else simply remove newline.\n",
    "            if '\\n' and '-' in token:\n",
    "                token = token.replace('\\n','').replace('-','')\n",
    "            else:\n",
    "                token = token.replace('\\n',' ')\n",
    "            if len(token) > 0:\n",
    "                new_doc.append(token)\n",
    "\n",
    "        return ' '.join(new_doc).strip()\n",
    "\n",
    "    def normalize_text(self,txt_blob):\n",
    "        txt_blob = self.newline_join(txt_blob)\n",
    "        txt_blob = self.newline_replace(txt_blob)\n",
    "        regex = r\"[\\W_]\"\n",
    "        txt_blob = re.sub(regex, \" \", txt_blob, 0)\n",
    "        txt_blob = ' '.join(txt_blob.split())\n",
    "        spcy_txt = nlp(txt_blob)\n",
    "        \n",
    "        spcy_txt_new = []\n",
    "        for token in spcy_txt:\n",
    "            if not(self.remove_stop and token.is_punct):\n",
    "                spcy_txt_new.append(token.lemma_)\n",
    "        return spcy_txt_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an infection could have spread from the bladder to the kidneys and then the antibiotics may have cured it (or possibly) hidden it, perhaps in the prostate. they are notoriously'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = qa[35][1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an infection could have spread from the bladder to the kidney and then the antibiotic may have cure -PRON- or possibly hide -PRON- perhaps in the prostate -PRON- be notoriously'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test text\n",
    "normalizer = QueryResponseNormalizer()\n",
    "' '.join(normalizer.normalize_text(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_sentence(txt,length=30):\n",
    "    \"\"\"\n",
    "    Shortens to 20 words.\n",
    "    \n",
    "    Input:\n",
    "        txt: a string\n",
    "    Output:\n",
    "        a string\n",
    "    \"\"\"\n",
    "    clean = ' '.join(str(txt).lower().replace(\"\\n\",\"\").split(' ')[:length])\n",
    "    \n",
    "    return ' '.join(clean.split())\n",
    "\n",
    "def only_original_interactions(query_response):\n",
    "    \"\"\"\n",
    "    Removes a query/response pair if the query has been seen before.\n",
    "    Input:\n",
    "        query_response: a list of tuples\n",
    "    Output:\n",
    "        A list of tuples\n",
    "    \"\"\"\n",
    "    query_response_new = []\n",
    "    original_question = set()\n",
    "    for tup in zip(*query_response):\n",
    "        if tup[0] not in original_question:\n",
    "            query_response_new.append(tup)\n",
    "            original_question.update(tup[0])\n",
    "    print('Originally {} chats. Removed {} chats from input.'.format(len(query_response),\n",
    "                                                                (len(query_response)-len(query_response_new))))\n",
    "\n",
    "    return query_response_new\n",
    "\n",
    "def retrieve_corpora(path_to_pickle):\n",
    "    data = pickle.load(open(path_to_pickle,'rb'))\n",
    "    qa = []\n",
    "    for i in list(data):\n",
    "        qa.append((i[0]['utterance'],i[1]['utterance']))\n",
    "    return qa\n",
    "\n",
    "data = retrieve_corpora('../data/all_responses_equal.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108825 chats total. Removed 108823 chats from input.\n"
     ]
    }
   ],
   "source": [
    "d = only_original_interactions(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num original pairs: 29722\n"
     ]
    }
   ],
   "source": [
    "qa_new = []\n",
    "original_question = set()\n",
    "for line in qa:\n",
    "    if line[0] not in original_question:\n",
    "        qa_new.append(line)\n",
    "        original_question.add(line[0])\n",
    "print('Num original pairs:',len(qa_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(qa_new,open('seq2seq_examples/fra-eng/all_responses_equal.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seq2seq_examples/fra-eng/all_responses_equal.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s -----+----- %s' % x for x in qa_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
