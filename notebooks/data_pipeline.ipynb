{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Purpose\n",
    "Currently being used to shortcut creating data for training.\n",
    "\n",
    "*However:* A more rigerous method for cleaning data and make a pipeline where changes can produce better results in training.\n",
    "\n",
    "Use as reference: https://www.safaribooksonline.com/library/view/hands-on-automated-machine/9781788629898/ccac8d45-a703-42b9-992c-d82eafafe94d.xhtml\n",
    "\n",
    "* use as example for text utilities: https://github.com/openai/finetune-transformer-lm/blob/master/text_utils.py\n",
    "\n",
    "\n",
    "\n",
    "### Pipeline considerations:\n",
    "* 1) Cleaning\n",
    "    * Stop words\n",
    "    * newline, tab \n",
    "    * Lowercasing\n",
    "* 2) Normalizing\n",
    "    * Splitting stemmed words into stem and removed portion\n",
    "    * Restricting the length of sentences (query and/or response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "punctuations = string.punctuation\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "query_response = pickle.load(open(\"../data/query_response_direct.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline_sentencize = spacy.load('en')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "class QueryResponseNormalizer:\n",
    "    \"\"\"\n",
    "    Able to initialize parameters for cleansing and normalizing both query and response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pass optional parameters to \n",
    "    def __init__(self,\n",
    "                remove_stop=False,\n",
    "                lowercase=True,\n",
    "                 remove_punct = True,\n",
    "                lem_stem = True,\n",
    "                num_sent_words = 100):\n",
    "        self.remove_stop = remove_stop\n",
    "        self.remove_punct = remove_punct\n",
    "        # optionally lowercase all words\n",
    "        self.lowercase = lowercase\n",
    "        # options are: None (don't do anything), stem (stem but remove extra) and stem_split (stem and add extra)\n",
    "        self.lem_stem = lem_stem\n",
    "        # limits the number of words (not tokens) in a query\n",
    "        self.num_sent_words = num_sent_words\n",
    "    \n",
    "    def newline_replace(self,txt_blob):\n",
    "        \"\"\"\n",
    "        Recursive function deals with multiple newline replacement.\n",
    "        \"\"\"\n",
    "        txt_blob = txt_blob.replace('\\n\\n','\\n')\n",
    "        if '\\n\\n' in txt_blob:\n",
    "            self.newline_replace(txt_blob)\n",
    "        else:\n",
    "            txt_blob = txt_blob.replace('\\n',' ')\n",
    "            return txt_blob  \n",
    "        \n",
    "    def newline_join(self,doc):\n",
    "        \"\"\"\n",
    "        Function deals with issue of having a hyphenatic word connected to a word after a newline.\n",
    "        \"\"\"\n",
    "        new_doc = []\n",
    "        for token in doc.split(' '):\n",
    "            # strip whitespace\n",
    "            token = token.strip()\n",
    "            # join words if necessary else simply remove newline.\n",
    "            if '\\n' and '-' in token:\n",
    "                token = token.replace('\\n','').replace('-','')\n",
    "            else:\n",
    "                token = token.replace('\\n',' ')\n",
    "            if len(token) > 0:\n",
    "                new_doc.append(token)\n",
    "\n",
    "        return ' '.join(new_doc).strip()\n",
    "\n",
    "    def normalize_text(self,txt_blob):\n",
    "        \"\"\"\n",
    "        Function deals with \n",
    "        \"\"\"\n",
    "        txt_blob = self.newline_join(txt_blob)\n",
    "        txt_blob = self.newline_replace(txt_blob)\n",
    "        regex = r\"[\\W_]\"\n",
    "        txt_blob = re.sub(regex, \" \", txt_blob, 0)\n",
    "        txt_blob = ' '.join(txt_blob.split())\n",
    "        spcy_txt = nlp(txt_blob)\n",
    "        \n",
    "        spcy_txt_new = []\n",
    "        for token in spcy_txt:\n",
    "            # optionally remove stops\n",
    "            if self.remove_stop and token.is_stop:\n",
    "                continue\n",
    "            elif self.remove_punct and token.is_punct:\n",
    "                continue\n",
    "            elif self.lem_stem:\n",
    "                spcy_txt_new.append(token.lemma_)\n",
    "            else:\n",
    "                spcy_txt_new.append(token.text)\n",
    "        return spcy_txt_new\n",
    "    \n",
    "    def sentencizer(self,txt_blob,num_sent=1):\n",
    "        \"\"\"\n",
    "        For now just returns first sentence.\n",
    "        \n",
    "        Input:\n",
    "            Raw text\n",
    "        \"\"\"\n",
    "        spcy_txt = nlp(txt_blob)\n",
    "        \n",
    "        return [sent.text for sent in spcy_txt.sents][:num_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm like the tree.  \n",
      "I'm like the tree\n"
     ]
    }
   ],
   "source": [
    "example = nlp(\" I'm like the tree.  I'm like the tree\")\n",
    "for s in example.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, I've had a mild sore throat for about 4 days. When I woke up yesterday morning the sore throat was worse than any strep I've ever dealt with. On top of that, because of the swelling of my tonsils, I haven't slept(it's now 6:22 am) because I just stop breathing in my sleep and wake up feeling like I've been holding my breath. I took DayQuil yesterday, plenty of acetaminophen and ibuprofen, and phenylephrine HCL hoping that would have some effect but my tonsils are bigger than ever. I can't eat, sleep, or drink anything and the amoxicillin hasn't had any effect(it's only been a day of antibiotics so far).  What are some remedies/other OTC medicines that will reduce the swelling? I've already tried gargling salt water, throat lozenges, tea(couldn't swallow It), ice cubes, and Ice packs on my swollen lymph nodes \n",
      "_______________________________________\n",
      "I'd say that's probably mononucleosis (i'm a last year medical student). Large tonsils. worse than any strep you've ever get, no effect with amoxicillin, trouble breathing and large painful lymph nodes. Yep, I'd say that. If so it doesn't really have a specific treatment except for acetaminophen and/or ibuprofen for fever and pain but you should get tested anyway. Also sometimes people with mono will get a skin rash when on amoxicillin. But not every one get's it. You should see a doc to confirm this and rule out other stuff like tonsil abcess or so.\n"
     ]
    }
   ],
   "source": [
    "q,r = query_response[43]\n",
    "print(q)\n",
    "print('_______________________________________')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'txt_blob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-6dcb9e88b250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryResponseNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_blob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlem_stem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'txt_blob'"
     ]
    }
   ],
   "source": [
    "# test text\n",
    "normalizer = QueryResponseNormalizer(txt_blob=q,remove_stop=True,lem_stem=False).normalize_text()\n",
    "dir(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, I've had a mild sore throat for about 4 days.\n",
      "so -PRON- ve mild sore throat 4 day\n",
      "--------------------------------------------\n",
      "I'd say that's probably mononucleosis (i'm a last year medical student).\n",
      "I d s probably mononucleosis m year medical student\n"
     ]
    }
   ],
   "source": [
    "# test text\n",
    "normalizer_q = QueryResponseNormalizer(lowercase=True,remove_stop=True,lem_stem=True)\n",
    "s_q = normalizer_q.sentencizer(q)\n",
    "print(s_q)\n",
    "print(' '.join(normalizer_q.normalize_text(s_q)))\n",
    "print('--------------------------------------------')\n",
    "normalizer_r = QueryResponseNormalizer(lowercase=True,remove_stop=True,lem_stem=False)\n",
    "r_q = normalizer_r.sentencizer(r)\n",
    "print(r_q)\n",
    "print(' '.join(normalizer_r.normalize_text(r_q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-2522c07eb840>\u001b[0m in \u001b[0;36mnormalize_text\u001b[0;34m(self, txt_blob)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mFunction\u001b[0m \u001b[0mdeals\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \"\"\"\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtxt_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewline_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mtxt_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewline_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"[\\W_]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-2522c07eb840>\u001b[0m in \u001b[0;36mnewline_join\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[1;32m     42\u001b[0m         \u001b[0mnew_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m# strip whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_response_new = []\n",
    "for (q,r) in query_response[:4000]:\n",
    "    if (len(str(q).strip().split()) >= 1) and (len(str(r).strip().split()) >= 1):\n",
    "        normalizer_q = QueryResponseNormalizer(lowercase=True,remove_stop=True,lem_stem=True)\n",
    "        s_q = normalizer_q.sentencizer(str(q),num_sent=2)\n",
    "        s_q = ' '.join(normalizer_q.normalize_text(s_q))\n",
    "\n",
    "        normalizer_r = QueryResponseNormalizer(lowercase=True,remove_stop=True,lem_stem=False)\n",
    "        s_r = normalizer_q.sentencizer(str(r),num_sent=1)\n",
    "        s_r = ' '.join(normalizer_q.normalize_text(s_r))\n",
    "        query_response_new.append((s_q,s_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pertinent facts\\n\\n', '*']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data from data creation option 2: Every comment is a question and answer\n",
    "pickle.dump(query_response_new,open( '../data/query_response_direct_one_sentence.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seq2seq_examples/fra-eng/all_responses_equal.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s -----+----- %s' % x for x in qa_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
