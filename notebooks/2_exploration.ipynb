{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Validate the structure of data especially the comments for which I was unable to find data dictionary and had to validate guesses about its structure.\n",
    "\n",
    "### References\n",
    "\n",
    "* Chatbot with TF: https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077\n",
    "* Good reference list for reading: http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\n",
    "* **Chameleon paper:**http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf\n",
    "* **Pandas pipelines:**https://medium.com/bigdatarepublic/integrating-pandas-and-scikit-learn-with-pipelines-f70eb6183696\n",
    "\n",
    "\n",
    "### Options:\n",
    "* Use categories from semtype tagging to identify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "import sys,os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500) # more columns displayed at once\n",
    "pd.options.display.max_colwidth = 200 # more of the text displayed at once\n",
    "\n",
    "# Custom\n",
    "from processing import tag_utterances\n",
    "from processing import load_sem_types\n",
    "\n",
    "## NLP\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body',\n",
       " 'score_hidden',\n",
       " 'archived',\n",
       " 'name',\n",
       " 'author',\n",
       " 'author_flair_text',\n",
       " 'downs',\n",
       " 'created_utc',\n",
       " 'subreddit_id',\n",
       " 'link_id',\n",
       " 'parent_id',\n",
       " 'score',\n",
       " 'retrieved_on',\n",
       " 'controversiality',\n",
       " 'gilded',\n",
       " 'id',\n",
       " 'subreddit',\n",
       " 'ups',\n",
       " 'distinguished',\n",
       " 'author_flair_css_class',\n",
       " 'removal_reason',\n",
       " 'is_clinician',\n",
       " 'tokenized_sents']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the columns\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (557648, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austinpowell/Google_Drive/kp_datascience/virtual_envs/nlp-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for a manlet such as yourself I'd recommend at least 70 oz of water daily and at least 7 hours of sleep. Cut down on red meat, smoking, and sodium intake and check back in a few days.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-Ai</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1513411674</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_7k5x2h</td>\n",
       "      <td>t3_7k5x2h</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514772e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>drbt2db</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you very much for answering!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-SY</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1445798103</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_3q697b</td>\n",
       "      <td>t1_cwcf958</td>\n",
       "      <td>2</td>\n",
       "      <td>1.447190e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cwcfjpr</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                       body  \\\n",
       "0  for a manlet such as yourself I'd recommend at least 70 oz of water daily and at least 7 hours of sleep. Cut down on red meat, smoking, and sodium intake and check back in a few days.    \n",
       "1                                                                                                                                                        Thank you very much for answering!   \n",
       "\n",
       "   score_hidden archived name author                     author_flair_text  \\\n",
       "0           NaN      NaN  NaN    -Ai  This user has not yet been verified.   \n",
       "1           NaN      NaN  NaN    -SY  This user has not yet been verified.   \n",
       "\n",
       "   downs  created_utc subreddit_id    link_id   parent_id  score  \\\n",
       "0    NaN   1513411674     t5_2xtuc  t3_7k5x2h   t3_7k5x2h      0   \n",
       "1    NaN   1445798103     t5_2xtuc  t3_3q697b  t1_cwcf958      2   \n",
       "\n",
       "   retrieved_on  controversiality  gilded       id subreddit  ups  \\\n",
       "0  1.514772e+09                 0       0  drbt2db   AskDocs  NaN   \n",
       "1  1.447190e+09                 0       0  cwcfjpr   AskDocs  2.0   \n",
       "\n",
       "  distinguished author_flair_css_class  removal_reason  \n",
       "0           NaN                default             NaN  \n",
       "1           NaN                default             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data = '../data/reddit_comments_askDocs_2014_to_2018_03.gz'\n",
    "df = pd.read_csv(path_to_data,dtype={'body':str,'score_hidden':float})\n",
    "print('Shape',df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary:\n",
    "Original post can be seen with the link_id\n",
    "* **link_id** Link to the page where the original thread was started\n",
    "    * **id** A posts id. The only reason this may not be unique\n",
    "    * **parent_id** These ids are pointing back towards the original post and represent a new comment that is replying directly towards the original posting. Can be considered a general comment. I.e. All posts with the same parent_id (following the \"_\") as the link_id are. E.g. If the link_id is \"t3_827pgt\", all parent_id's with \"827pgt\" are pointing towards that original post.\n",
    "\n",
    "* **subreddit_id** and **subreddit** Irrelevant\n",
    "\n",
    "\n",
    "### Documentation:\n",
    "* Figuring out how to organize a reply and response is tricky:\n",
    "    * On one hand, the people asking the questions should be the non-clinician folks and the clinicians answering the questions. But it is a community forum so people can chime in with ''answers'' whenever. Ansd they might be correct.\n",
    "     *  Also, it is likely that clinicians will ask follow-up questions if it is more debate than answer so perhaps drilling down into a solution\n",
    "     * This latest point gets at how to utilize the id's and parent_ids\n",
    "     \n",
    "* <b>There are 2 reasonably straightforward ways of getting to the reply/response//question/answer pairs:</b>\n",
    "    * 1) Treat each main post as the 1 question (then randomly select 1 response or have each comment in that thread be a response to the same question repeated for each response\n",
    "    * 2) Treat each id (starting with the original thread) as the question and each subsequent post that has that id as its parent (**i.e. parent_id**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',df.shape[0])\n",
    "print('Number of unique posters:',len(df.author.unique()))\n",
    "print()\n",
    "print('Number of link_ids:',len(df.link_id.unique()))\n",
    "print('Number of parent_ids:',len(df.parent_id.unique()))\n",
    "print('Number of ids:',len(df.id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proportion of unique links to num rows:',len(df['link_id'].unique())/df.shape[0])\n",
    "print('Proportion of unique ids to num rows:',len(df['id'].unique())/df.shape[0])\n",
    "print('Proportion of unique parent ids to num rows:',len(df['parent_id'].unique())/df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Entity Extraction with UMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e09206a31951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mabs_path_umls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/austinpowell/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mabs_path_data_umls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/austinpowell/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS_db'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path_umls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/QuickUMLS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mquickumls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuickUMLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuickUMLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path_data_umls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "# UMLS\n",
    "# Set absolute path the QuickUMLS Server\n",
    "abs_path_umls = '/Users/austinpowell/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS'\n",
    "abs_path_data_umls = '/Users/austinpowell/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS_db'\n",
    "sys.path.append(abs_path_umls+'/QuickUMLS')\n",
    "from quickumls import QuickUMLS\n",
    "tagger = QuickUMLS(abs_path_data_umls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic types\n",
    "sem_type_dict = load_sem_types('../data/SemGroups_2013.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She's had it for 8 months, we've never had any issues with it before.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tagger.nlp(df['body'].iloc[4])\n",
    "t.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 19, 'end': 25, 'ngram': 'months', 'term': 'month', 'cui': 'C1561542', 'similarity': 0.75, 'semtypes': {'T170'}, 'preferred': 1}] \n",
      "\n",
      "[{'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C0033213', 'similarity': 0.75, 'semtypes': {'T033'}, 'preferred': 1}, {'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C1706387', 'similarity': 0.75, 'semtypes': {'T170'}, 'preferred': 1}] \n",
      "\n",
      "{'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C0033213', 'similarity': 0.75, 'semtypes': {'T033'}, 'preferred': 1}\n",
      "{'start': 47, 'end': 53, 'ngram': 'issues', 'term': 'issue', 'cui': 'C1706387', 'similarity': 0.75, 'semtypes': {'T170'}, 'preferred': 1}\n"
     ]
    }
   ],
   "source": [
    "t = tagger.nlp(t.text)\n",
    "\n",
    "s = t\n",
    "matches= tagger.match(s, best_match=True, ignore_syntax=False)\n",
    "for match in matches:\n",
    "    dir(match)\n",
    "    print(match,'\\n')\n",
    "for m in match:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-56b3b167801b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sem_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/SemGroups_2013.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'semtypes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "sd = load_sem_types('../data/SemGroups_2013.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T170'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 19, 25, 'month', 'C1561542', 0.75, {'Intellectual Product'}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from processing import tag_utterances\n",
    "from processing import load_sem_types\n",
    "\n",
    "tag_utterances(1, t.text, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557648, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over every document\n",
      "Documents processed: 0\n",
      "Documents processed: 10000\n",
      "Documents processed: 20000\n",
      "Documents processed: 30000\n",
      "Documents processed: 40000\n",
      "Documents processed: 50000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 60000\n",
      "Documents processed: 70000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 80000\n",
      "Documents processed: 90000\n",
      "Documents processed: 100000\n",
      "Documents processed: 110000\n",
      "Documents processed: 120000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 130000\n",
      "Documents processed: 140000\n",
      "Documents processed: 150000\n",
      "Documents processed: 160000\n",
      "Documents processed: 170000\n",
      "Documents processed: 180000\n",
      "Documents processed: 190000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 200000\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Argument 'string' has incorrect type (expected str, got float)\n",
      "Documents processed: 210000\n",
      "Documents processed: 220000\n",
      "Documents processed: 230000\n",
      "Documents processed: 240000\n",
      "Documents processed: 250000\n",
      "Documents processed: 260000\n",
      "Documents processed: 270000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/Projects/Automated-Health-Responses/notebooks/processing.py\u001b[0m in \u001b[0;36mtag_utterances\u001b[0;34m(id, txt, tagger, sems)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstpwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Run the UMLS tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmatches\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_syntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/quickumls.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, text, best_match, ignore_syntax)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_all_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/quickumls.py\u001b[0m in \u001b[0;36m_get_all_matches\u001b[0;34m(self, ngrams)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mprev_cui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mngram_cands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mss_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mngram_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/toolbox.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, term)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_string_for_db_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/kp_datascience/doctor_notes/ontology/UMLS/QuickUMLS/simstring/simstring.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0m__del__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_simstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Iterating over every document')\n",
    "#Iterate over every document and extract the concepts\n",
    "i=-1        \n",
    "result = []\n",
    "for idx,doc in  enumerate(df['body']):\n",
    "\n",
    "    if idx % 10000 == 0:\n",
    "        print(\"Documents processed: {}\".format(idx))\n",
    "    try:\n",
    "        i+=1\n",
    "        annotations = tag_utterances(i,doc,tagger)\n",
    "        result.extend(annotations)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "df_matches = pd.DataFrame(data=result, columns =['document','start','end','term','cui','similarity','semtypes'])\n",
    "df_matches.sort_values(by=['document','start'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>...</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>removal_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for a manlet such as yourself I'd recommend at...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-Ai</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1513411674</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_7k5x2h</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514772e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>drbt2db</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you very much for answering!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-SY</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1445798103</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_3q697b</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.447190e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cwcfjpr</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never been tested for that.  I was hoping the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-o2</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1461952470</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_4gz1fi</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.463777e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d2mce34</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She said her constant abdominal pain is a 6, t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05P</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1504214332</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_6x9jk0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.504553e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dme9lzr</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She's had it for 8 months, we've never had any...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05P</td>\n",
       "      <td>This user has not yet been verified.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1504217835</td>\n",
       "      <td>t5_2xtuc</td>\n",
       "      <td>t3_6x9jk0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.504554e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dmecohs</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body score_hidden archived  \\\n",
       "0  for a manlet such as yourself I'd recommend at...          NaN      NaN   \n",
       "1                 Thank you very much for answering!          NaN      NaN   \n",
       "2  Never been tested for that.  I was hoping the ...          NaN      NaN   \n",
       "3  She said her constant abdominal pain is a 6, t...          NaN      NaN   \n",
       "4  She's had it for 8 months, we've never had any...          NaN      NaN   \n",
       "\n",
       "  name author                     author_flair_text  downs  created_utc  \\\n",
       "0  NaN    -Ai  This user has not yet been verified.    NaN   1513411674   \n",
       "1  NaN    -SY  This user has not yet been verified.    NaN   1445798103   \n",
       "2  NaN    -o2  This user has not yet been verified.    NaN   1461952470   \n",
       "3  NaN    05P  This user has not yet been verified.    NaN   1504214332   \n",
       "4  NaN    05P  This user has not yet been verified.    NaN   1504217835   \n",
       "\n",
       "  subreddit_id    link_id      ...       score  retrieved_on  \\\n",
       "0     t5_2xtuc  t3_7k5x2h      ...           0  1.514772e+09   \n",
       "1     t5_2xtuc  t3_3q697b      ...           2  1.447190e+09   \n",
       "2     t5_2xtuc  t3_4gz1fi      ...           1  1.463777e+09   \n",
       "3     t5_2xtuc  t3_6x9jk0      ...           1  1.504553e+09   \n",
       "4     t5_2xtuc  t3_6x9jk0      ...           1  1.504554e+09   \n",
       "\n",
       "   controversiality  gilded       id subreddit  ups  distinguished  \\\n",
       "0                 0       0  drbt2db   AskDocs  NaN            NaN   \n",
       "1                 0       0  cwcfjpr   AskDocs  2.0            NaN   \n",
       "2                 0       0  d2mce34   AskDocs  1.0            NaN   \n",
       "3                 0       0  dme9lzr   AskDocs  NaN            NaN   \n",
       "4                 0       0  dmecohs   AskDocs  NaN            NaN   \n",
       "\n",
       "  author_flair_css_class removal_reason  \n",
       "0                default            NaN  \n",
       "1                default            NaN  \n",
       "2                default            NaN  \n",
       "3                default            NaN  \n",
       "4                default            NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Journal\n",
    "\n",
    "* **Creating training data**\n",
    "    * A lot of decisions have to be made when creating the conversational data. We could just give line by line\n",
    "    * Even deciding what an utterance is can be difficult. Is it split by sentence? Do we assume that people are going to be generally giving one answer?\n",
    "    * Should the title or actual post be used as the query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
